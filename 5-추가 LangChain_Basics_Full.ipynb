{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b0f9c58",
   "metadata": {
    "id": "2b0f9c58"
   },
   "source": [
    "\n",
    "## **2. LangChain ê¸°ë³¸ ì‚¬ìš©ë²•**\n",
    "\n",
    "LangChainì˜ í•µì‹¬ ê°œë…ì€ **Runnable**ì„ ì´ìš©í•´ ë‹¨ê³„ë¥¼ **ì—°ê²°(Chaining)** í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "ì•„ë˜ ì˜ˆì œëŠ” **OpenAI APIë¥¼ LangChainìœ¼ë¡œ ì‰½ê²Œ í˜¸ì¶œí•˜ëŠ” ë°©ë²•**ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cd73ad",
   "metadata": {
    "id": "d7cd73ad"
   },
   "source": [
    "\n",
    "### âœ… **1ï¸âƒ£ LangChain ì„¤ì¹˜**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03d7b9a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5939,
     "status": "ok",
     "timestamp": 1744950151808,
     "user": {
      "displayName": "kc s",
      "userId": "11321376974888545210"
     },
     "user_tz": -540
    },
    "id": "03d7b9a1",
    "outputId": "27274e3c-53b7-4dcf-bfe4-8be72d788abf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (0.3.23)\n",
      "Requirement already satisfied: langchain-openai in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (0.3.14)\n",
      "Requirement already satisfied: openai in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (1.75.0)\n",
      "Requirement already satisfied: dotenv in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (0.9.9)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain) (0.3.54)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain) (0.3.32)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain) (2.11.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from dotenv) (1.1.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (24.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain langchain-openai openai dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ff9bb1",
   "metadata": {
    "id": "06ff9bb1"
   },
   "source": [
    "\n",
    "### âœ… **2ï¸âƒ£ OpenAI APIë¥¼ í™œìš©í•œ ê¸°ë³¸ LangChain ì˜ˆì œ**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90a538f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17404,
     "status": "ok",
     "timestamp": 1744950191788,
     "user": {
      "displayName": "kc s",
      "userId": "11321376974888545210"
     },
     "user_tz": -540
    },
    "id": "90a538f7",
    "outputId": "4833a290-5e05-4ee5-fef6-b887acecb9c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChainì€ ì£¼ë¡œ ìì—°ì–´ ì²˜ë¦¬(NLP) ë° ì¸ê³µì§€ëŠ¥(AI) ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. LangChainì€ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” ë„êµ¬ì™€ ì»´í¬ë„ŒíŠ¸ë¥¼ ì œê³µí•˜ì—¬, ë³µì¡í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì´ë‚˜ íŒŒì´í”„ë¼ì¸ì„ ì‰½ê²Œ êµ¬ì¶•í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.\n",
      "\n",
      "ì£¼ìš” íŠ¹ì§•ê³¼ ê¸°ëŠ¥ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "1. **ì–¸ì–´ ëª¨ë¸ê³¼ì˜ í†µí•©**  \n",
      "   OpenAI, Hugging Face, Anthropic ë“± ë‹¤ì–‘í•œ LLM ì„œë¹„ìŠ¤ì™€ ì‰½ê²Œ ì—°ë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "2. **ì²´ì¸(Chain) êµ¬ì„±**  \n",
      "   ì—¬ëŸ¬ ê°œì˜ ì–¸ì–´ ëª¨ë¸ í˜¸ì¶œ, API ìš”ì²­, ë°ì´í„° ë³€í™˜ ë‹¨ê³„ë¥¼ í•˜ë‚˜ì˜ íë¦„ìœ¼ë¡œ ì—°ê²°í•´ì„œ ë³µì¡í•œ ì‘ì—…ì„ ìë™í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "3. **í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê´€ë¦¬**  \n",
      "   ì¬ì‚¬ìš© ê°€ëŠ¥í•˜ê³  ë™ì ìœ¼ë¡œ ë³€í˜• ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì‘ì„±, ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "4. **ë©”ëª¨ë¦¬ ê´€ë¦¬**  \n",
      "   ì‚¬ìš©ìì™€ì˜ ëŒ€í™” ë‚´ìš©ì„ ì €ì¥í•˜ê³  ì¬í™œìš©í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì„ ì œê³µí•´ ëŒ€í™”í˜• ì• í”Œë¦¬ì¼€ì´ì…˜ ì‘ì„±ì— ìš©ì´í•©ë‹ˆë‹¤.\n",
      "\n",
      "5. **ë„êµ¬ ë° ë°ì´í„° ì†ŒìŠ¤ ì—°ê²°**  \n",
      "   ë°ì´í„°ë² ì´ìŠ¤, ê²€ìƒ‰ ì—”ì§„, ë¬¸ì„œ ì¸ë±ìŠ¤ ë“± ì™¸ë¶€ ë°ì´í„°ì™€ ì—°ë™í•˜ì—¬ ì§€ëŠ¥í˜• ì‘ë‹µì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ìš”ì•½í•˜ë©´, LangChainì€ LLMì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ë³´ë‹¤ ì‰½ê³  êµ¬ì¡°ì ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ëŠ” í”„ë ˆì„ì›Œí¬ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì±—ë´‡, ìë™ ë¬¸ì„œ ìƒì„±, ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ, ë³µì¡í•œ ì˜ì‚¬ê²°ì • ì§€ì› ë“± ë‹¤ì–‘í•œ AI ê¸°ë°˜ ì„œë¹„ìŠ¤ ê°œë°œì— í™œìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# OpenAI ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", api_key=openai_api_key)\n",
    "\n",
    "# ì§ˆë¬¸í•˜ê¸°\n",
    "response = llm.invoke(\"LangChainì´ ë­ì•¼?\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e030c91f",
   "metadata": {
    "id": "e030c91f"
   },
   "source": [
    "\n",
    "ğŸ’¡ **LangChainì„ ì‚¬ìš©í•˜ë©´ OpenAI APIë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ëŠ” ê²ƒë³´ë‹¤ í›¨ì”¬ ê°„ë‹¨í•˜ê³  ì§ê´€ì ì…ë‹ˆë‹¤.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aae21b4",
   "metadata": {
    "id": "6aae21b4"
   },
   "source": [
    "\n",
    "## **3. LangChainìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©í•˜ê¸°**\n",
    "\n",
    "LangChainì—ì„œëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ì‰½ê²Œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì‚¬ìš©í•˜ë©´ **ê³ ì •ëœ ì§ˆë¬¸ í˜•ì‹ì„ ì‰½ê²Œ ì¬ì‚¬ìš© ê°€ëŠ¥**í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2be7d521",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1744950406353,
     "user": {
      "displayName": "kc s",
      "userId": "11321376974888545210"
     },
     "user_tz": -540
    },
    "id": "2be7d521",
    "outputId": "5528352c-4986-494f-cb6e-f63f0649a0ce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:280: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain import hub\n",
    "\n",
    "# RAGì—ì„œ ì‚¬ìš©í•  LangChain í”„ë¡¬í”„íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# ì˜ˆì œ í”„ë¡¬í”„íŠ¸ ì¶œë ¥\n",
    "print(rag_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3a7dbc",
   "metadata": {
    "id": "df3a7dbc"
   },
   "source": [
    "\n",
    "ğŸ’¡ **LangChainì€ \"í”„ë¡¬í”„íŠ¸ë¥¼ ì§ì ‘ ê´€ë¦¬\"í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•©ë‹ˆë‹¤.**\n",
    "\n",
    "â†’ **ê° ì§ˆë¬¸ë§ˆë‹¤ ë‹¤ë¥¸ í˜•ì‹ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ë§Œë“¤ í•„ìš” ì—†ì´, ë¯¸ë¦¬ ì •ì˜ëœ í…œí”Œë¦¿ì„ í™œìš© ê°€ëŠ¥!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab6e55e",
   "metadata": {
    "id": "6ab6e55e"
   },
   "source": [
    "\n",
    "## **4. LangChainìœ¼ë¡œ ì²´ì´ë‹(Chaining) ì‚¬ìš©í•˜ê¸°**\n",
    "\n",
    "LangChainì€ ì—¬ëŸ¬ ê°œì˜ ë‹¨ê³„ë¥¼ ì—°ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì•„ë˜ëŠ” **í”„ë¡¬í”„íŠ¸ ìƒì„± â†’ LLM í˜¸ì¶œ**ì„ ì²´ì´ë‹í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cab32d79",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1122,
     "status": "ok",
     "timestamp": 1744950528566,
     "user": {
      "displayName": "kc s",
      "userId": "11321376974888545210"
     },
     "user_tz": -540
    },
    "id": "cab32d79",
    "outputId": "bfb452fb-f62a-4c8d-d8ce-d88237886fbb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:280: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChainì˜ ì¥ì ì€ AI ê°œë°œì— ìœ ìš©í•œ ë„êµ¬ë¼ëŠ” ì ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê°œë°œìëŠ” ë³µì¡í•œ AI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë³´ë‹¤ ì‰½ê²Œ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. êµ¬ì²´ì ì¸ ê¸°ëŠ¥ì´ë‚˜ íŠ¹ì§•ì— ëŒ€í•œ ì •ë³´ëŠ” ì£¼ì–´ì§„ ë¬¸ë§¥ì— ì—†ìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain import hub\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# OpenAI ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", api_key=openai_api_key)\n",
    "\n",
    "# LangChain ì²´ì´ë‹ - í”„ë¡¬í”„íŠ¸ì™€ ëª¨ë¸ì„ ì—°ê²°\n",
    "pipeline = rag_prompt | llm\n",
    "\n",
    "# ì‹¤í–‰\n",
    "response = pipeline.invoke({\n",
    "    \"context\": \"LangChainì€ AI ê°œë°œì— ìœ ìš©í•œ ë„êµ¬ì…ë‹ˆë‹¤.\",\n",
    "    \"question\": \"LangChainì˜ ì¥ì ì€?\"\n",
    "})\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39cba68",
   "metadata": {
    "id": "b39cba68"
   },
   "source": [
    "## ğŸ”§ ì‹¤ìŠµ 1: ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©í•´ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e76a10af",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1744950586914,
     "user": {
      "displayName": "kc s",
      "userId": "11321376974888545210"
     },
     "user_tz": -540
    },
    "id": "e76a10af",
    "outputId": "87bc70af-68fb-4b8a-f3f2-fa250178a1a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë‹¹ì‹ ì€ ì¹œì ˆí•œ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì§ˆë¬¸ì— ì„±ì‹¤íˆ ë‹µë³€í•˜ì„¸ìš”: LangChainì˜ ì¥ì ì€?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"ë‹¹ì‹ ì€ ì¹œì ˆí•œ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì§ˆë¬¸ì— ì„±ì‹¤íˆ ë‹µë³€í•˜ì„¸ìš”: {question}\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "formatted_prompt = prompt.format(question=\"LangChainì˜ ì¥ì ì€?\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02d20043",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1744950605996,
     "user": {
      "displayName": "kc s",
      "userId": "11321376974888545210"
     },
     "user_tz": -540
    },
    "id": "02d20043",
    "outputId": "cc50a3fb-b56b-4900-b017-8a39235fda5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì§ˆë¬¸ì— ì•„ë˜ ì •ë³´ë¥¼ ë°˜ì˜í•´ì„œ ë‹µí•´ì£¼ì„¸ìš”.\n",
      "ë°°ê²½ì •ë³´: LangChainì€ LLMì„ ì‰½ê²Œ ì—°ê²°í•©ë‹ˆë‹¤.\n",
      "ì§ˆë¬¸: ì™œ LangChainì„ ì¨ì•¼ í•˜ë‚˜ìš”?\n"
     ]
    }
   ],
   "source": [
    "# ë‹¤ì¤‘ ë³€ìˆ˜ í…œí”Œë¦¿\n",
    "multi_template = (\n",
    "    \"ì§ˆë¬¸ì— ì•„ë˜ ì •ë³´ë¥¼ ë°˜ì˜í•´ì„œ ë‹µí•´ì£¼ì„¸ìš”.\\n\"\n",
    "    \"ë°°ê²½ì •ë³´: {context}\\n\"\n",
    "    \"ì§ˆë¬¸: {question}\"\n",
    ")\n",
    "multi_prompt = PromptTemplate.from_template(multi_template)\n",
    "\n",
    "print(multi_prompt.format(context=\"LangChainì€ LLMì„ ì‰½ê²Œ ì—°ê²°í•©ë‹ˆë‹¤.\", question=\"ì™œ LangChainì„ ì¨ì•¼ í•˜ë‚˜ìš”?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcb4441",
   "metadata": {
    "id": "5fcb4441"
   },
   "source": [
    "## ğŸ§ª ì‹¤ìŠµ 2: ë‹¤ì–‘í•œ OpenAI ëª¨ë¸ ë¶ˆëŸ¬ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "309015bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1987,
     "status": "ok",
     "timestamp": 1744950617986,
     "user": {
      "displayName": "kc s",
      "userId": "11321376974888545210"
     },
     "user_tz": -540
    },
    "id": "309015bd",
    "outputId": "984054ae-d8a0-469c-a9b6-3270671990aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4.1-nanoo ì‘ë‹µ: LangChainì€ ìì—°ì–´ ì²˜ë¦¬(NLP) ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ë‹¤ì–‘í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•  ìˆ˜ ìˆë„ë¡ ë•ëŠ” í”„ë ˆì„ì›Œí¬ ë° ë„êµ¬ ëª¨ìŒì…ë‹ˆë‹¤. ì£¼ë¡œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLMs)ì„ í†µí•©í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì§€ëŠ¥í˜• ì‹œìŠ¤í…œì„ ì‰½ê²Œ êµ¬ì¶•í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "**LangChainì˜ ì£¼ìš” íŠ¹ì§•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:**\n",
      "\n",
      "1. **ëª¨ë“ˆí™”ëœ êµ¬ì¡°:** ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë‹¤ì–‘í•œ êµ¬ì„± ìš”ì†Œ(ì²´ì¸, ì—ì´ì „íŠ¸, ë„êµ¬ ë“±)ë¥¼ ì‰½ê²Œ ê²°í•©í•˜ê³  ì¬ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
      "2. **ëŒ€í™”í˜• ì• í”Œë¦¬ì¼€ì´ì…˜ ì§€ì›:** ì±—ë´‡, ëŒ€í™”í˜• ì—ì´ì „íŠ¸ ë“±ì„ ê°œë°œí•˜ëŠ” ë° ìœ ìš©í•˜ë©°, ì‚¬ìš©ìì™€ì˜ ìƒí˜¸ì‘ìš©ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ë•ìŠµë‹ˆë‹¤.\n",
      "3. **ì™¸ë¶€ ë„êµ¬ ì—°ë™:** ë°ì´í„°ë² ì´ìŠ¤, ê²€ìƒ‰ ì—”ì§„, API ë“± ì™¸ë¶€ ë„êµ¬ì™€ ì‰½ê²Œ í†µí•©í•˜ì—¬ ë” ê°•ë ¥í•œ ê¸°ëŠ¥ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "4. **ê°œë°œ ìƒì‚°ì„± í–¥ìƒ:** ë³µì¡í•œ ì–¸ì–´ ëª¨ë¸ í™œìš© ë¡œì§ì„ ê°„ë‹¨í•˜ê²Œ êµ¬ì„±í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ì–´ ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘ê³¼ ê°œë°œì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.\n",
      "\n",
      "**ìš”ì•½:**  \n",
      "LangChainì€ ê°•ë ¥í•œ ì–¸ì–´ ëª¨ë¸ ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë³´ë‹¤ ì‰½ê³  íš¨ìœ¨ì ìœ¼ë¡œ ê°œë°œí•  ìˆ˜ ìˆë„ë¡ ë•ëŠ” í”„ë ˆì„ì›Œí¬ë¡œ, ìì—°ì–´ ì´í•´ì™€ ìƒì„±, ì—°ì†ì„± ìœ ì§€, ì™¸ë¶€ ë„êµ¬ì™€ì˜ ì—°ë™ ë“±ì„ ì§€ì›í•©ë‹ˆë‹¤.\n",
      "\n",
      "í˜¹ì‹œ ë” êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì›í•˜ì‹œê±°ë‚˜ íŠ¹ì • ê¸°ëŠ¥ì— ëŒ€í•´ ê¶ê¸ˆí•˜ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”!\n"
     ]
    }
   ],
   "source": [
    "# gpt-4.1-nano ëª¨ë¸ë¡œ ë³€ê²½í•´ì„œ ê²°ê³¼ ë¹„êµ\n",
    "llm_gpt_nano = ChatOpenAI(model=\"gpt-4.1-nano\", api_key=openai_api_key)\n",
    "response_nano = llm_gpt_nano.invoke(\"LangChainì´ ë­ì•¼?\")\n",
    "print(\"gpt-4.1-nanoo ì‘ë‹µ:\", response_nano.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd168659",
   "metadata": {
    "id": "fd168659"
   },
   "source": [
    "## ğŸ§  ì‹¤ìŠµ 3: Memoryë¡œ ëŒ€í™” ìƒíƒœ ìœ ì§€í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98e8b55b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2755,
     "status": "ok",
     "timestamp": 1744950642714,
     "user": {
      "displayName": "kc s",
      "userId": "11321376974888545210"
     },
     "user_tz": -540
    },
    "id": "98e8b55b",
    "outputId": "83e52b90-f157-4da1-d93f-4906139f9f77"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-643e49096e34>:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages=True)\n",
      "<ipython-input-8-643e49096e34>:5: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[]\n",
      "Human: ì•ˆë…•?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ì•ˆë…•í•˜ì„¸ìš”! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”. ì˜¤ëŠ˜ ê¸°ë¶„ì€ ì–´ë– ì‹ ê°€ìš”? ê¶ê¸ˆí•œ ê²ƒì´ë‚˜ ì´ì•¼ê¸°í•˜ê³  ì‹¶ì€ ì£¼ì œê°€ ìˆìœ¼ë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”!\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='ì•ˆë…•?', additional_kwargs={}, response_metadata={}), AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”. ì˜¤ëŠ˜ ê¸°ë¶„ì€ ì–´ë– ì‹ ê°€ìš”? ê¶ê¸ˆí•œ ê²ƒì´ë‚˜ ì´ì•¼ê¸°í•˜ê³  ì‹¶ì€ ì£¼ì œê°€ ìˆìœ¼ë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”!', additional_kwargs={}, response_metadata={})]\n",
      "Human: ë‚´ê°€ ë°©ê¸ˆ ë­ë¼ê³  í–ˆì§€?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ë‹¹ì‹ ì€ \"ì•ˆë…•?\"ì´ë¼ê³  ì¸ì‚¬í•˜ì…¨ì–´ìš”. ê°„ë‹¨í•˜ì§€ë§Œ ì¹œê·¼í•œ ì¸ì‚¬ë¼ì„œ ì €ë„ ë°˜ê°‘ê²Œ ë‹µí–ˆë‹µë‹ˆë‹¤! ë‹¤ë¥¸ ì§ˆë¬¸ì´ë‚˜ ê¶ê¸ˆí•œ ì  ìˆìœ¼ë©´ ì–¸ì œë“ ì§€ ë¬¼ì–´ë³´ì„¸ìš”.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
    "\n",
    "print(conversation.invoke(\"ì•ˆë…•?\")['response'])\n",
    "print(conversation.invoke(\"ë‚´ê°€ ë°©ê¸ˆ ë­ë¼ê³  í–ˆì§€?\")['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c805b042",
   "metadata": {
    "id": "c805b042"
   },
   "source": [
    "## ğŸ”— ì‹¤ìŠµ 4: ì²´ì´ë‹ í™•ì¥ ì‹¤ìŠµ (Parser ì¶”ê°€ ë“±)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4be2df7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 449,
     "status": "ok",
     "timestamp": 1744950690705,
     "user": {
      "displayName": "kc s",
      "userId": "11321376974888545210"
     },
     "user_tz": -540
    },
    "id": "a4be2df7",
    "outputId": "37e0fb01-0e6c-4851-96a9-1a7c450b06a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChainì€ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# íŒŒì´í”„ë¼ì¸ êµ¬ì„±: í”„ë¡¬í”„íŠ¸ â†’ LLM â†’ íŒŒì‹±\n",
    "pipeline = rag_prompt | llm | StrOutputParser()\n",
    "\n",
    "output = pipeline.invoke({\"context\": \"LangChainì€ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\", \"question\": \"LangChainì´ ë­”ê°€ìš”?\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde03999",
   "metadata": {
    "id": "bde03999"
   },
   "source": [
    "## ğŸ§® ì‹¤ìŠµ 5: RunnableLambdaë¡œ ì „ì²˜ë¦¬ ì¶”ê°€í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa7a584c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1175,
     "status": "ok",
     "timestamp": 1744950749062,
     "user": {
      "displayName": "kc s",
      "userId": "11321376974888545210"
     },
     "user_tz": -540
    },
    "id": "fa7a584c",
    "outputId": "d8ee43bf-fc14-4b3a-aa2b-6963789dc5e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANGCHAINì€ ë‹¤ì–‘í•œ ì‘ì—…ì„ ìë™í™”í•˜ê³  íš¨ìœ¨ì„±ì„ ë†’ì´ê¸° ë•Œë¬¸ì— ì¢‹ìŠµë‹ˆë‹¤. ì‚¬ìš©ì´ ì‰½ê³  ì—¬ëŸ¬ ê¸°ëŠ¥ì„ í†µí•©í•  ìˆ˜ ìˆì–´ ìœ ìš©í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# ì‚¬ìš©ì ì…ë ¥ì„ ì „ì²˜ë¦¬í•˜ëŠ” ëŒë‹¤\n",
    "preprocess = RunnableLambda(lambda x: {\"context\": x[\"context\"].upper(), \"question\": x[\"question\"]})\n",
    "\n",
    "# ì²´ì¸ êµ¬ì„±: ì „ì²˜ë¦¬ â†’ í”„ë¡¬í”„íŠ¸ â†’ ëª¨ë¸ â†’ íŒŒì„œ\n",
    "pipeline = preprocess | rag_prompt | llm | StrOutputParser()\n",
    "\n",
    "response = pipeline.invoke({\"context\": \"LangChainì€ ìœ ìš©í•œ ë„êµ¬ì…ë‹ˆë‹¤.\", \"question\": \"ì™œ ì¢‹ì€ê°€ìš”?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8b9912",
   "metadata": {
    "id": "bd8b9912"
   },
   "source": [
    "## ğŸ” ì‹¤ìŠµ 6: LLM ì‘ë‹µ í›„ ìš”ì•½/í•„í„°ë§ ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baacd32f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 834,
     "status": "ok",
     "timestamp": 1744950809699,
     "user": {
      "displayName": "kc s",
      "userId": "11321376974888545210"
     },
     "user_tz": -540
    },
    "id": "baacd32f",
    "outputId": "dbba3f79-190f-4f51-ee25-ba82af48517d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChainì€ í…ìŠ¤íŠ¸ ê¸°ë°˜ ì²˜ë¦¬, ì²´ì´ë‹, í”„ë¡¬í”„... (ìš”ì•½ë¨)\n"
     ]
    }
   ],
   "source": [
    "# í›„ì²˜ë¦¬ìš© ëŒë‹¤ ì¶”ê°€\n",
    "postprocess = RunnableLambda(lambda x: x[:30] + \"... (ìš”ì•½ë¨)\" if isinstance(x, str) else x)\n",
    "\n",
    "# ì „ì²´ ì²´ì¸: í”„ë¡¬í”„íŠ¸ â†’ LLM â†’ íŒŒì‹± â†’ í›„ì²˜ë¦¬\n",
    "full_chain = rag_prompt | llm | StrOutputParser() | postprocess\n",
    "\n",
    "result = full_chain.invoke({\"context\": \"LangChainì€ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ê°€ì§„ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. í…ìŠ¤íŠ¸ ê¸°ë°˜ ì²˜ë¦¬, ì²´ì´ë‹, í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ ë“±ì„ ì§€ì›í•©ë‹ˆë‹¤.\",\n",
    "                            \"question\": \"LangChainì˜ ê¸°ëŠ¥ì„ ìš”ì•½í•´ì¤˜\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rR3UiaWfv-uE",
   "metadata": {
    "id": "rR3UiaWfv-uE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
