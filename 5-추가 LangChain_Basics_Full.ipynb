{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b0f9c58",
   "metadata": {
    "id": "2b0f9c58"
   },
   "source": [
    "\n",
    "## **2. LangChain 기본 사용법**\n",
    "\n",
    "LangChain의 핵심 개념은 **Runnable**을 이용해 단계를 **연결(Chaining)** 하는 것입니다.\n",
    "\n",
    "아래 예제는 **OpenAI API를 LangChain으로 쉽게 호출하는 방법**을 보여줍니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cd73ad",
   "metadata": {
    "id": "d7cd73ad"
   },
   "source": [
    "\n",
    "### ✅ **1️⃣ LangChain 설치**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03d7b9a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5939,
     "status": "ok",
     "timestamp": 1744950151808,
     "user": {
      "displayName": "kc s",
      "userId": "11321376974888545210"
     },
     "user_tz": -540
    },
    "id": "03d7b9a1",
    "outputId": "27274e3c-53b7-4dcf-bfe4-8be72d788abf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (0.3.23)\n",
      "Requirement already satisfied: langchain-openai in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (0.3.14)\n",
      "Requirement already satisfied: openai in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (1.75.0)\n",
      "Requirement already satisfied: dotenv in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (0.9.9)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain) (0.3.54)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain) (0.3.32)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain) (2.11.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from dotenv) (1.1.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (24.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/llm/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain langchain-openai openai dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ff9bb1",
   "metadata": {
    "id": "06ff9bb1"
   },
   "source": [
    "\n",
    "### ✅ **2️⃣ OpenAI API를 활용한 기본 LangChain 예제**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90a538f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17404,
     "status": "ok",
     "timestamp": 1744950191788,
     "user": {
      "displayName": "kc s",
      "userId": "11321376974888545210"
     },
     "user_tz": -540
    },
    "id": "90a538f7",
    "outputId": "4833a290-5e05-4ee5-fef6-b887acecb9c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain은 주로 자연어 처리(NLP) 및 인공지능(AI) 분야에서 사용되는 오픈소스 프레임워크입니다. LangChain은 대형 언어 모델(LLM)을 효과적으로 활용할 수 있도록 도와주는 도구와 컴포넌트를 제공하여, 복잡한 애플리케이션이나 파이프라인을 쉽게 구축할 수 있게 해줍니다.\n",
      "\n",
      "주요 특징과 기능은 다음과 같습니다:\n",
      "\n",
      "1. **언어 모델과의 통합**  \n",
      "   OpenAI, Hugging Face, Anthropic 등 다양한 LLM 서비스와 쉽게 연동할 수 있습니다.\n",
      "\n",
      "2. **체인(Chain) 구성**  \n",
      "   여러 개의 언어 모델 호출, API 요청, 데이터 변환 단계를 하나의 흐름으로 연결해서 복잡한 작업을 자동화할 수 있습니다.\n",
      "\n",
      "3. **프롬프트 템플릿 관리**  \n",
      "   재사용 가능하고 동적으로 변형 가능한 프롬프트 템플릿을 작성, 관리할 수 있습니다.\n",
      "\n",
      "4. **메모리 관리**  \n",
      "   사용자와의 대화 내용을 저장하고 재활용하는 메커니즘을 제공해 대화형 애플리케이션 작성에 용이합니다.\n",
      "\n",
      "5. **도구 및 데이터 소스 연결**  \n",
      "   데이터베이스, 검색 엔진, 문서 인덱스 등 외부 데이터와 연동하여 지능형 응답을 만들 수 있습니다.\n",
      "\n",
      "요약하면, LangChain은 LLM을 활용한 애플리케이션 개발을 보다 쉽고 구조적으로 만들어주는 프레임워크라고 할 수 있습니다. 챗봇, 자동 문서 생성, 질의응답 시스템, 복잡한 의사결정 지원 등 다양한 AI 기반 서비스 개발에 활용되고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 환경 변수 로드\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# OpenAI 모델 불러오기\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", api_key=openai_api_key)\n",
    "\n",
    "# 질문하기\n",
    "response = llm.invoke(\"LangChain이 뭐야?\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e030c91f",
   "metadata": {
    "id": "e030c91f"
   },
   "source": [
    "\n",
    "💡 **LangChain을 사용하면 OpenAI API를 직접 호출하는 것보다 훨씬 간단하고 직관적입니다.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aae21b4",
   "metadata": {
    "id": "6aae21b4"
   },
   "source": [
    "\n",
    "## **3. LangChain으로 프롬프트 템플릿 사용하기**\n",
    "\n",
    "LangChain에서는 프롬프트를 쉽게 관리할 수 있습니다.\n",
    "\n",
    "프롬프트 템플릿을 사용하면 **고정된 질문 형식을 쉽게 재사용 가능**합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2be7d521",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1744950406353,
     "user": {
      "displayName": "kc s",
      "userId": "11321376974888545210"
     },
     "user_tz": -540
    },
    "id": "2be7d521",
    "outputId": "5528352c-4986-494f-cb6e-f63f0649a0ce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:280: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain import hub\n",
    "\n",
    "# RAG에서 사용할 LangChain 프롬프트 불러오기\n",
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# 예제 프롬프트 출력\n",
    "print(rag_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3a7dbc",
   "metadata": {
    "id": "df3a7dbc"
   },
   "source": [
    "\n",
    "💡 **LangChain은 \"프롬프트를 직접 관리\"할 수 있도록 지원합니다.**\n",
    "\n",
    "→ **각 질문마다 다른 형식의 프롬프트를 만들 필요 없이, 미리 정의된 템플릿을 활용 가능!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab6e55e",
   "metadata": {
    "id": "6ab6e55e"
   },
   "source": [
    "\n",
    "## **4. LangChain으로 체이닝(Chaining) 사용하기**\n",
    "\n",
    "LangChain은 여러 개의 단계를 연결할 수 있습니다.\n",
    "\n",
    "아래는 **프롬프트 생성 → LLM 호출**을 체이닝하는 코드입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cab32d79",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1122,
     "status": "ok",
     "timestamp": 1744950528566,
     "user": {
      "displayName": "kc s",
      "userId": "11321376974888545210"
     },
     "user_tz": -540
    },
    "id": "cab32d79",
    "outputId": "bfb452fb-f62a-4c8d-d8ce-d88237886fbb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:280: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain의 장점은 AI 개발에 유용한 도구라는 점입니다. 이를 통해 개발자는 복잡한 AI 애플리케이션을 보다 쉽게 구축할 수 있습니다. 구체적인 기능이나 특징에 대한 정보는 주어진 문맥에 없습니다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain import hub\n",
    "\n",
    "# 프롬프트 템플릿 불러오기\n",
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# OpenAI 모델 불러오기\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", api_key=openai_api_key)\n",
    "\n",
    "# LangChain 체이닝 - 프롬프트와 모델을 연결\n",
    "pipeline = rag_prompt | llm\n",
    "\n",
    "# 실행\n",
    "response = pipeline.invoke({\n",
    "    \"context\": \"LangChain은 AI 개발에 유용한 도구입니다.\",\n",
    "    \"question\": \"LangChain의 장점은?\"\n",
    "})\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39cba68",
   "metadata": {
    "id": "b39cba68"
   },
   "source": [
    "## 🔧 실습 1: 다양한 프롬프트 템플릿 사용해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e76a10af",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1744950586914,
     "user": {
      "displayName": "kc s",
      "userId": "11321376974888545210"
     },
     "user_tz": -540
    },
    "id": "e76a10af",
    "outputId": "87bc70af-68fb-4b8a-f3f2-fa250178a1a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "당신은 친절한 도우미입니다. 질문에 성실히 답변하세요: LangChain의 장점은?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"당신은 친절한 도우미입니다. 질문에 성실히 답변하세요: {question}\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "formatted_prompt = prompt.format(question=\"LangChain의 장점은?\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02d20043",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1744950605996,
     "user": {
      "displayName": "kc s",
      "userId": "11321376974888545210"
     },
     "user_tz": -540
    },
    "id": "02d20043",
    "outputId": "cc50a3fb-b56b-4900-b017-8a39235fda5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문에 아래 정보를 반영해서 답해주세요.\n",
      "배경정보: LangChain은 LLM을 쉽게 연결합니다.\n",
      "질문: 왜 LangChain을 써야 하나요?\n"
     ]
    }
   ],
   "source": [
    "# 다중 변수 템플릿\n",
    "multi_template = (\n",
    "    \"질문에 아래 정보를 반영해서 답해주세요.\\n\"\n",
    "    \"배경정보: {context}\\n\"\n",
    "    \"질문: {question}\"\n",
    ")\n",
    "multi_prompt = PromptTemplate.from_template(multi_template)\n",
    "\n",
    "print(multi_prompt.format(context=\"LangChain은 LLM을 쉽게 연결합니다.\", question=\"왜 LangChain을 써야 하나요?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcb4441",
   "metadata": {
    "id": "5fcb4441"
   },
   "source": [
    "## 🧪 실습 2: 다양한 OpenAI 모델 불러보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "309015bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1987,
     "status": "ok",
     "timestamp": 1744950617986,
     "user": {
      "displayName": "kc s",
      "userId": "11321376974888545210"
     },
     "user_tz": -540
    },
    "id": "309015bd",
    "outputId": "984054ae-d8a0-469c-a9b6-3270671990aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4.1-nanoo 응답: LangChain은 자연어 처리(NLP) 기술을 활용하여 다양한 애플리케이션을 개발할 수 있도록 돕는 프레임워크 및 도구 모음입니다. 주로 대형 언어 모델(LLMs)을 통합하고, 이를 기반으로 한 지능형 시스템을 쉽게 구축할 수 있도록 설계되었습니다.\n",
      "\n",
      "**LangChain의 주요 특징은 다음과 같습니다:**\n",
      "\n",
      "1. **모듈화된 구조:** 데이터를 처리하는 다양한 구성 요소(체인, 에이전트, 도구 등)를 쉽게 결합하고 재사용할 수 있도록 설계되어 있습니다.\n",
      "2. **대화형 애플리케이션 지원:** 챗봇, 대화형 에이전트 등을 개발하는 데 유용하며, 사용자와의 상호작용을 자연스럽게 처리할 수 있도록 돕습니다.\n",
      "3. **외부 도구 연동:** 데이터베이스, 검색 엔진, API 등 외부 도구와 쉽게 통합하여 더 강력한 기능을 구현할 수 있습니다.\n",
      "4. **개발 생산성 향상:** 복잡한 언어 모델 활용 로직을 간단하게 구성할 수 있도록 도와주어 빠른 프로토타이핑과 개발을 가능하게 합니다.\n",
      "\n",
      "**요약:**  \n",
      "LangChain은 강력한 언어 모델 기반 애플리케이션을 보다 쉽고 효율적으로 개발할 수 있도록 돕는 프레임워크로, 자연어 이해와 생성, 연속성 유지, 외부 도구와의 연동 등을 지원합니다.\n",
      "\n",
      "혹시 더 구체적인 정보를 원하시거나 특정 기능에 대해 궁금하시면 말씀해 주세요!\n"
     ]
    }
   ],
   "source": [
    "# gpt-4.1-nano 모델로 변경해서 결과 비교\n",
    "llm_gpt_nano = ChatOpenAI(model=\"gpt-4.1-nano\", api_key=openai_api_key)\n",
    "response_nano = llm_gpt_nano.invoke(\"LangChain이 뭐야?\")\n",
    "print(\"gpt-4.1-nanoo 응답:\", response_nano.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd168659",
   "metadata": {
    "id": "fd168659"
   },
   "source": [
    "## 🧠 실습 3: Memory로 대화 상태 유지하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98e8b55b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2755,
     "status": "ok",
     "timestamp": 1744950642714,
     "user": {
      "displayName": "kc s",
      "userId": "11321376974888545210"
     },
     "user_tz": -540
    },
    "id": "98e8b55b",
    "outputId": "83e52b90-f157-4da1-d93f-4906139f9f77"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-643e49096e34>:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages=True)\n",
      "<ipython-input-8-643e49096e34>:5: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[]\n",
      "Human: 안녕?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "안녕하세요! 만나서 반가워요. 오늘 기분은 어떠신가요? 궁금한 것이나 이야기하고 싶은 주제가 있으면 언제든지 말씀해 주세요!\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='안녕?', additional_kwargs={}, response_metadata={}), AIMessage(content='안녕하세요! 만나서 반가워요. 오늘 기분은 어떠신가요? 궁금한 것이나 이야기하고 싶은 주제가 있으면 언제든지 말씀해 주세요!', additional_kwargs={}, response_metadata={})]\n",
      "Human: 내가 방금 뭐라고 했지?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "당신은 \"안녕?\"이라고 인사하셨어요. 간단하지만 친근한 인사라서 저도 반갑게 답했답니다! 다른 질문이나 궁금한 점 있으면 언제든지 물어보세요.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
    "\n",
    "print(conversation.invoke(\"안녕?\")['response'])\n",
    "print(conversation.invoke(\"내가 방금 뭐라고 했지?\")['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c805b042",
   "metadata": {
    "id": "c805b042"
   },
   "source": [
    "## 🔗 실습 4: 체이닝 확장 실습 (Parser 추가 등)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4be2df7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 449,
     "status": "ok",
     "timestamp": 1744950690705,
     "user": {
      "displayName": "kc s",
      "userId": "11321376974888545210"
     },
     "user_tz": -540
    },
    "id": "a4be2df7",
    "outputId": "37e0fb01-0e6c-4851-96a9-1a7c450b06a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain은 프레임워크입니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# 파이프라인 구성: 프롬프트 → LLM → 파싱\n",
    "pipeline = rag_prompt | llm | StrOutputParser()\n",
    "\n",
    "output = pipeline.invoke({\"context\": \"LangChain은 프레임워크입니다.\", \"question\": \"LangChain이 뭔가요?\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde03999",
   "metadata": {
    "id": "bde03999"
   },
   "source": [
    "## 🧮 실습 5: RunnableLambda로 전처리 추가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa7a584c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1175,
     "status": "ok",
     "timestamp": 1744950749062,
     "user": {
      "displayName": "kc s",
      "userId": "11321376974888545210"
     },
     "user_tz": -540
    },
    "id": "fa7a584c",
    "outputId": "d8ee43bf-fc14-4b3a-aa2b-6963789dc5e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANGCHAIN은 다양한 작업을 자동화하고 효율성을 높이기 때문에 좋습니다. 사용이 쉽고 여러 기능을 통합할 수 있어 유용합니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# 사용자 입력을 전처리하는 람다\n",
    "preprocess = RunnableLambda(lambda x: {\"context\": x[\"context\"].upper(), \"question\": x[\"question\"]})\n",
    "\n",
    "# 체인 구성: 전처리 → 프롬프트 → 모델 → 파서\n",
    "pipeline = preprocess | rag_prompt | llm | StrOutputParser()\n",
    "\n",
    "response = pipeline.invoke({\"context\": \"LangChain은 유용한 도구입니다.\", \"question\": \"왜 좋은가요?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8b9912",
   "metadata": {
    "id": "bd8b9912"
   },
   "source": [
    "## 🔍 실습 6: LLM 응답 후 요약/필터링 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baacd32f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 834,
     "status": "ok",
     "timestamp": 1744950809699,
     "user": {
      "displayName": "kc s",
      "userId": "11321376974888545210"
     },
     "user_tz": -540
    },
    "id": "baacd32f",
    "outputId": "dbba3f79-190f-4f51-ee25-ba82af48517d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain은 텍스트 기반 처리, 체이닝, 프롬프... (요약됨)\n"
     ]
    }
   ],
   "source": [
    "# 후처리용 람다 추가\n",
    "postprocess = RunnableLambda(lambda x: x[:30] + \"... (요약됨)\" if isinstance(x, str) else x)\n",
    "\n",
    "# 전체 체인: 프롬프트 → LLM → 파싱 → 후처리\n",
    "full_chain = rag_prompt | llm | StrOutputParser() | postprocess\n",
    "\n",
    "result = full_chain.invoke({\"context\": \"LangChain은 다양한 기능을 가진 프레임워크입니다. 텍스트 기반 처리, 체이닝, 프롬프트 관리 등을 지원합니다.\",\n",
    "                            \"question\": \"LangChain의 기능을 요약해줘\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rR3UiaWfv-uE",
   "metadata": {
    "id": "rR3UiaWfv-uE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
