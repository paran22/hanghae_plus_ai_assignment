{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tkat-RaWc1u4"
   },
   "source": [
    "# MLP 실습\n",
    "\n",
    "이번 실습에서는 PyTorch에서 MLP 및 backpropagation을 구현하여 XOR 문제에 학습해봅니다. 먼저 필요한 library들을 import합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 8556,
     "status": "ok",
     "timestamp": 1723364650516,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "DEJFJkL6qHB9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UC0DqhmtdBuK"
   },
   "source": [
    "그 다음 공유된 notebook과 똑같은 결과를 얻기 위한 코드를 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1723364650518,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "nnXpVhJXbRzw"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "seed = 7777\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmAh1sEJdKUN"
   },
   "source": [
    "위의 코드에서는 seed 고정과 `cudnn.deterministic`, `cudnn.benchmark`를 조정하였습니다. 자세한 사항들은 https://pytorch.org/docs/stable/notes/randomness.html에서 찾아볼 수 있습니다.\n",
    "\n",
    "다음은 이전 실습과 같은 코드로 XOR data를 생성하는 과정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1723364650518,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "SsEdD6T7qLJH",
    "outputId": "46a82aae-b239-4f92-b1f8-2745101a69bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [0., 0.],\n",
    "    [0., 1.],\n",
    "    [1., 0.],\n",
    "    [1., 1.]\n",
    "])\n",
    "y = torch.tensor([0, 1, 1, 0])\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sSAu0HKdlRc"
   },
   "source": [
    "Data를 만들었으니 이제 MLP를 구현해야 합니다. 여기서 알아둬야할 점들은 다음과 같습니다:\n",
    "- PyTorch에서는 우리가 학습하고자 하는 함수 $f$를 보통 `torch.nn.Module` class를 상속받은 class로 구현합니다.\n",
    "- `torch.nn.Module` class는 abstract class로, `def forward`를 구현하도록 abstractmethod를 제공합니다. 이 method는 $f(x)$, 즉 함수의 출력에 해당됩니다.\n",
    "- PyTorch에서는 선형함수를 `torch.nn.Linear` class로 구현할 수 있습니다.\n",
    "- 마찬가지로 ReLU도 `torch.nn.ReLU`로 제공하고 있습니다.\n",
    "\n",
    "위의 점들을 활용하여 2-layer MLP를 구현한 결과는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1723364650518,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "KZe7xFjMqfc0"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "  def __init__(self, d, d_prime):\n",
    "    super().__init__()\n",
    "\n",
    "    self.layer1 = nn.Linear(d, d_prime)\n",
    "    self.layer2 = nn.Linear(d_prime, 1)\n",
    "    self.act = nn.ReLU()\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x: (n, d)\n",
    "    x = self.layer1(x)  # (n, d_prime)\n",
    "    x = self.act(x)     # (n, d_prime)\n",
    "    x = self.layer2(x)  # (n, 1)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "model = Model(2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjPzBGayeZ2f"
   },
   "source": [
    "우리가 원하는 것은 $x \\in \\mathbb{R}^{n \\times d}$가 입력으로 들어왔을 때 첫 번째 선형 함수를 거쳤을 때의 dimension이 $\\mathbb{R}^{n \\times d'}$이 되면서 두 번째 선형 함수를 거쳤을 때는 $\\mathbb{R}^{n \\times 1}$이 되기를 원합니다.\n",
    "첫 번째 선형함수는 `self.layer1 = nn.Linear(d, d_prime)`로 구현할 수 있으며, `nn.Linear`의 첫 번째 인자는 입력으로 들어오는 tensor의 마지막 dimension, 두 번째 인자는 출력되는 tensor의 마지막 dimension을 뜻합니다.\n",
    "이 정보를 이용하면 두 번째 선형함수도 `self.layer2 = nn.Linear(d_prime, 1)`로 구현할 수 있습니다.\n",
    "마찬가지로 ReLU도 `nn.ReLU()`로 선언할 수 있습니다.\n",
    "\n",
    "이제 $f(x)$에 대한 구현은 `def forward(self, x)`에서 할 수 있습니다.\n",
    "생성자에서 선언한 세 개의 layer들을 순차적으로 지나면 우리가 원하던 결과를 얻을 수 있습니다.\n",
    "여기서도 shape의 변화를 돌려보지 않고 예측하는 것이 실제로 구현하고 디버깅할 때 중요하게 여겨집니다.\n",
    "\n",
    "마지막 줄에서는 입력 dimension이 2이고, 중간 dimension이 10이 되는 2-layer MLP에 대한 object를 생성하였습니다.\n",
    "다음은 PyTorch에서 gradient descent를 어떻게 구현하는지 살펴봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4678,
     "status": "ok",
     "timestamp": 1723364655186,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "NpygQ87grSJV"
   },
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HraI34csgTYL"
   },
   "source": [
    "PyTorch는 다양한 update 알고리즘들을 `torch.optim`에서 제공하고 있습니다.\n",
    "우리는 gradient descent를 사용할 것이기 때문에 `SGD` class를 불러옵니다.\n",
    "`SGD`는 첫 번째 인자로 업데이트를 할 parameter들의 list를 받습니다. 예를 들어 선형 함수에서의 $w, b$가 있습니다.\n",
    "PyTorch의 `nn.Module` class는 이러한 것들을 잘 정의해주기 때문에 `model.parameters()`의 형식으로 넘겨주기만 하면 됩니다.\n",
    "두 번째 인자는 learning rate로, 이전의 gradient descent에서 사용하던 learning rate와 똑같은 역할을 가지고 있습니다.\n",
    "\n",
    "다음은 실제 학습 코드를 구현하며, backpropagation이 어떻게 이루어지는지 살펴봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1723364655186,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "Pa6fA_ZUFI-0"
   },
   "outputs": [],
   "source": [
    "def train(n_epochs, model, optimizer, x, y):\n",
    "  for e in range(n_epochs):\n",
    "    model.zero_grad()\n",
    "\n",
    "    y_pred = model(x)\n",
    "    loss = (y_pred[:, 0] - y).pow(2).sum()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {e:3d} | Loss: {loss}\")\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "heaHQYZVhBAt"
   },
   "source": [
    "`model(x)`를 통해 우리가 구현한 `forward` 함수를 사용할 수 있습니다. 그리고 이를 이용하여 MSE loss로 `model`을 평가할 수 있습니다.\n",
    "Gradient descent의 구현은 다음 세 가지 부분에서 진행되고 있습니다:\n",
    "- 본격적으로 학습이 진행되기 전에 `model.zero_grad()`를 실행합니다. 이는 각 parameter의 gradient 값이 저장되어 있을 수도 있기 때문에 지워주는 기능을 수행합니다.\n",
    "- loss를 계산한 후, `loss.backward()`를 진행합니다. backward 함수를 실행하면 `model`에 있는 모든 parameter들의 `loss`에 대한 gradient를 계산하게 됩니다.\n",
    "- 마지막으로 계산한 gradient들을 가지고 parameter들을 update하는 것은 `optimizer.step()`을 통해 진행하게 됩니다. optimizer는 이전에 인자로 받았던 `model.parameters()`에 해당하는 parameter들만 update를 진행하게 됩니다.\n",
    "\n",
    "위의 코드로 학습을 진행한 코드는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1723364655187,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "zFk-josgBSj7",
    "outputId": "10343a67-4055-4b7d-e103-34a1d043d01c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Loss: 2.8124639987945557\n",
      "Epoch   1 | Loss: 2.309718132019043\n",
      "Epoch   2 | Loss: 1.6600806713104248\n",
      "Epoch   3 | Loss: 0.9176856279373169\n",
      "Epoch   4 | Loss: 0.8310231566429138\n",
      "Epoch   5 | Loss: 0.7857465147972107\n",
      "Epoch   6 | Loss: 0.7469707131385803\n",
      "Epoch   7 | Loss: 0.7105731964111328\n",
      "Epoch   8 | Loss: 0.679701566696167\n",
      "Epoch   9 | Loss: 0.6505993604660034\n",
      "Epoch  10 | Loss: 0.6153720617294312\n",
      "Epoch  11 | Loss: 0.5817742347717285\n",
      "Epoch  12 | Loss: 0.5496706962585449\n",
      "Epoch  13 | Loss: 0.5309392213821411\n",
      "Epoch  14 | Loss: 0.500799834728241\n",
      "Epoch  15 | Loss: 0.4737371802330017\n",
      "Epoch  16 | Loss: 0.45748329162597656\n",
      "Epoch  17 | Loss: 0.4339975118637085\n",
      "Epoch  18 | Loss: 0.4096483290195465\n",
      "Epoch  19 | Loss: 0.38796404004096985\n",
      "Epoch  20 | Loss: 0.3663795590400696\n",
      "Epoch  21 | Loss: 0.3485005497932434\n",
      "Epoch  22 | Loss: 0.3266391158103943\n",
      "Epoch  23 | Loss: 0.3139018714427948\n",
      "Epoch  24 | Loss: 0.28671738505363464\n",
      "Epoch  25 | Loss: 0.2610698640346527\n",
      "Epoch  26 | Loss: 0.23623432219028473\n",
      "Epoch  27 | Loss: 0.21415168046951294\n",
      "Epoch  28 | Loss: 0.2061079442501068\n",
      "Epoch  29 | Loss: 0.1822202503681183\n",
      "Epoch  30 | Loss: 0.1610347330570221\n",
      "Epoch  31 | Loss: 0.14815261960029602\n",
      "Epoch  32 | Loss: 0.1299654245376587\n",
      "Epoch  33 | Loss: 0.12578868865966797\n",
      "Epoch  34 | Loss: 0.11440988630056381\n",
      "Epoch  35 | Loss: 0.1180308610200882\n",
      "Epoch  36 | Loss: 0.11135035008192062\n",
      "Epoch  37 | Loss: 0.09905607998371124\n",
      "Epoch  38 | Loss: 0.10490959882736206\n",
      "Epoch  39 | Loss: 0.08657150715589523\n",
      "Epoch  40 | Loss: 0.08665228635072708\n",
      "Epoch  41 | Loss: 0.06649560481309891\n",
      "Epoch  42 | Loss: 0.07306350767612457\n",
      "Epoch  43 | Loss: 0.06029697507619858\n",
      "Epoch  44 | Loss: 0.05810358375310898\n",
      "Epoch  45 | Loss: 0.06295822560787201\n",
      "Epoch  46 | Loss: 0.0699714720249176\n",
      "Epoch  47 | Loss: 0.056175682693719864\n",
      "Epoch  48 | Loss: 0.06725442409515381\n",
      "Epoch  49 | Loss: 0.05403407663106918\n",
      "Epoch  50 | Loss: 0.07560449838638306\n",
      "Epoch  51 | Loss: 0.05889705568552017\n",
      "Epoch  52 | Loss: 0.05406183376908302\n",
      "Epoch  53 | Loss: 0.04623538255691528\n",
      "Epoch  54 | Loss: 0.05982695519924164\n",
      "Epoch  55 | Loss: 0.04889903590083122\n",
      "Epoch  56 | Loss: 0.05164049565792084\n",
      "Epoch  57 | Loss: 0.04841793328523636\n",
      "Epoch  58 | Loss: 0.06241704523563385\n",
      "Epoch  59 | Loss: 0.051214709877967834\n",
      "Epoch  60 | Loss: 0.05042730271816254\n",
      "Epoch  61 | Loss: 0.051399894058704376\n",
      "Epoch  62 | Loss: 0.06424340605735779\n",
      "Epoch  63 | Loss: 0.052223727107048035\n",
      "Epoch  64 | Loss: 0.051350995898246765\n",
      "Epoch  65 | Loss: 0.04424889385700226\n",
      "Epoch  66 | Loss: 0.055919285863637924\n",
      "Epoch  67 | Loss: 0.046582743525505066\n",
      "Epoch  68 | Loss: 0.04642249643802643\n",
      "Epoch  69 | Loss: 0.04015394300222397\n",
      "Epoch  70 | Loss: 0.05007017403841019\n",
      "Epoch  71 | Loss: 0.0421726293861866\n",
      "Epoch  72 | Loss: 0.042124029248952866\n",
      "Epoch  73 | Loss: 0.03611918166279793\n",
      "Epoch  74 | Loss: 0.04397265613079071\n",
      "Epoch  75 | Loss: 0.037268321961164474\n",
      "Epoch  76 | Loss: 0.037098608911037445\n",
      "Epoch  77 | Loss: 0.03176155686378479\n",
      "Epoch  78 | Loss: 0.03148927167057991\n",
      "Epoch  79 | Loss: 0.027080848813056946\n",
      "Epoch  80 | Loss: 0.026670530438423157\n",
      "Epoch  81 | Loss: 0.023186251521110535\n",
      "Epoch  82 | Loss: 0.027390506118535995\n",
      "Epoch  83 | Loss: 0.023494239896535873\n",
      "Epoch  84 | Loss: 0.022942233830690384\n",
      "Epoch  85 | Loss: 0.019843922927975655\n",
      "Epoch  86 | Loss: 0.019305385649204254\n",
      "Epoch  87 | Loss: 0.016771724447607994\n",
      "Epoch  88 | Loss: 0.016230974346399307\n",
      "Epoch  89 | Loss: 0.014146372675895691\n",
      "Epoch  90 | Loss: 0.013615883886814117\n",
      "Epoch  91 | Loss: 0.011900360696017742\n",
      "Epoch  92 | Loss: 0.011394334957003593\n",
      "Epoch  93 | Loss: 0.009984707459807396\n",
      "Epoch  94 | Loss: 0.009513582102954388\n",
      "Epoch  95 | Loss: 0.008357474580407143\n",
      "Epoch  96 | Loss: 0.007927577942609787\n",
      "Epoch  97 | Loss: 0.006981097161769867\n",
      "Epoch  98 | Loss: 0.006595136597752571\n",
      "Epoch  99 | Loss: 0.0058211879804730415\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "model = train(n_epochs, model, optimizer, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCMcj41Ghu7w"
   },
   "source": [
    "Linear regression과는 달리 잘 수렴하는 모습을 보여주고 있습니다. 특히, 우리가 직접 gradient나 gradient descent를 구현하지 않아도 주어진 data를 잘 학습하는 코드를 PyTorch를 통해 구현할 수 있었습니다. 마지막으로 예측 결과를 살펴봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1723364655187,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "IggGP969Bh-w",
    "outputId": "d555310d-7c24-4f40-c80b-571f44a217fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0208],\n",
      "        [1.0484],\n",
      "        [1.0156],\n",
      "        [0.0496]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "print(model(x))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GuAOyBfKiTJL"
   },
   "source": [
    "매우 정확한 예측 결과를 보여주고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1723364655187,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "2zAy7YgFDMgx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO0Gd7VoUSDzxEwJov/s6Wg",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (welcomedl)",
   "language": "python",
   "name": "welcomedl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
