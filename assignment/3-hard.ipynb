{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [3주차] 심화과제 - Pre-trained 모델로 효율적인 NLP 모델 학습하기\n",
    "\n",
    "#### Q1) 어떤 task를 선택하셨나요?\n",
    "- MNLI task\n",
    "MNLI는 두 문장이 주어졌을 때 논리적으로 연결이 되어 있는지, 서로 모순되는지, 아니면 아예 무관한지 분류하는 문제이다.\n",
    "BERT는 sentence pair classification에 특화되어 있어 MNLI와 같이 두 문장 간의 복잡한 의미 관계를 파악하는데 효과적이다.\n",
    "BERT와 비슷한 성능을 가지면서 사이즈는 작은 DistilBERT를 사용하였다.\n",
    "\n",
    "#### Q2) 모델은 어떻게 설계하셨나요? 설계한 모델의 입력과 출력 형태가 어떻게 되나요?\n",
    "> 모델의 입력과 출력 형태 또는 shape을 정확하게 기술\n",
    "\n",
    "\n",
    "#### Q3) 실제로 pre-trained 모델을 fine-tuning했을 때 loss curve은 어떻게 그려지나요? 그리고 pre-train 하지 않은 Transformer를 학습했을 때와 어떤 차이가 있나요? \n",
    "> 비교 metric은 loss curve, accuracy, 또는 test data에 대한 generalization 성능 등을 활용.\n",
    "> +)이외에도 기계 번역 같은 문제에서 활용하는 BLEU 등의 metric을 마음껏 활용 가능\n",
    "- \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install tqdm boto3 requests regex sentencepiece sacremoses datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DistilBERT pre-training 때 사용한 tokenizer를 불러온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# DistilBERT 모델용 tokenizer 로드 (pretrained)\n",
    "# 이 tokenizer는 문장을 토큰화해서 모델이 이해할 수 있는 input_ids로 변환해줌\n",
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle 데이터셋을 다운로드 받는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"thedevastator/unlocking-language-understanding-with-the-multin\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "# 1. 데이터 로드\n",
    "def load_mnli_data(path):\n",
    "    train_path = os.path.join(path, 'multinli_1.0_train.txt')\n",
    "    val_path = os.path.join(path, 'multinli_1.0_dev_matched.txt')\n",
    "    \n",
    "    # tab으로 구분된 텍스트 파일 읽기\n",
    "    train_df = pd.read_csv(train_path, sep='\\t')\n",
    "    val_df = pd.read_csv(val_path, sep='\\t')\n",
    "    \n",
    "    # 필요한 컬럼만 선택\n",
    "    columns = ['sentence1', 'sentence2', 'gold_label']\n",
    "    train_df = train_df[columns]\n",
    "    val_df = val_df[columns]\n",
    "    \n",
    "    return train_df, val_df\n",
    "\n",
    "# 2. MNLI 데이터셋 클래스 정의\n",
    "class MNLIDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # 레이블 매핑\n",
    "        self.label_map = {\n",
    "            'entailment': 0,\n",
    "            'contradiction': 1,\n",
    "            'neutral': 2\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        # 텍스트 인코딩\n",
    "        encoding = self.tokenizer(\n",
    "            row['sentence1'],\n",
    "            row['sentence2'],\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # 배치 차원 제거\n",
    "        encoding = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        \n",
    "        # 레이블 변환\n",
    "        label = self.label_map.get(row['gold_label'], -1)  # -1은 invalid 레이블용\n",
    "        encoding['labels'] = torch.tensor(label)\n",
    "        \n",
    "        return encoding\n",
    "\n",
    "# 3. 데이터 준비 및 데이터로더 설정\n",
    "def prepare_dataloaders(path, batch_size=16):\n",
    "    # 토크나이저 초기화\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    \n",
    "    # 데이터 로드\n",
    "    train_df, val_df = load_mnli_data(path)\n",
    "    \n",
    "    # 유효하지 않은 레이블 제거\n",
    "    train_df = train_df[train_df['gold_label'].isin(['entailment', 'contradiction', 'neutral'])]\n",
    "    val_df = val_df[val_df['gold_label'].isin(['entailment', 'contradiction', 'neutral'])]\n",
    "    \n",
    "    # 데이터셋 생성\n",
    "    train_dataset = MNLIDataset(train_df, tokenizer)\n",
    "    val_dataset = MNLIDataset(val_df, tokenizer)\n",
    "    \n",
    "    # 데이터로더 생성\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "# 4. 사용 예시\n",
    "if __name__ == \"__main__\":\n",
    "    # kagglehub에서 다운로드 받은 경로 사용\n",
    "    path = kagglehub.dataset_download(\"thedevastator/unlocking-language-understanding-with-the-multin\")\n",
    "    \n",
    "    # 데이터로더 생성\n",
    "    train_loader, val_loader = prepare_dataloaders(path)\n",
    "    \n",
    "    # 데이터 형태 확인\n",
    "    for batch in train_loader:\n",
    "        print(\"입력 데이터 형태:\")\n",
    "        print(f\"Input IDs shape: {batch['input_ids'].shape}\")\n",
    "        print(f\"Attention mask shape: {batch['attention_mask'].shape}\")\n",
    "        print(f\"Labels shape: {batch['labels'].shape}\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
