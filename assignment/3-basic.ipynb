{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbgz49PvHhLt"
      },
      "source": [
        "# [3주차] 기본과제 - DistilBERT로 뉴스 기사 분류 모델 학습하기\n",
        "\n",
        "- [x] AG_News dataset 준비\n",
        "\t- Huggingface dataset의 `fancyzhx/ag_news`를 load\n",
        "\t- `collate_fn` 함수에 다음 수정사항들을 반영\n",
        "    - Truncation과 관련된 부분들을 삭제\n",
        "- [x] Classifier output, loss function, accuracy function 변경\n",
        "\t- 뉴스 기사 분류 문제는 binary classification이 아닌 일반적인 classification 문제입니다. MNIST 과제에서 했던 것 처럼 `nn.CrossEntropyLoss` 를 추가하고 `TextClassifier`의 출력 차원을 잘 조정하여 task를 풀 수 있도록 수정\n",
        "\t- 그리고 정확도를 재는 `accuracy` 함수도 classification에 맞춰 수정\n",
        "- [x]  학습 결과 report\n",
        "    - DistilBERT 실습과 같이 매 epoch 마다의 train loss를 출력하고 최종 모델의 test accuracy를 report 첨부\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1LqgujQUbv6X",
        "outputId": "30ae8095-637a-480c-e13c-65c723f5e83b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (1.37.28)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.11/dist-packages (0.1.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: botocore<1.38.0,>=1.37.28 in /usr/local/lib/python3.11/dist-packages (from boto3) (1.37.28)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from boto3) (0.11.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore<1.38.0,>=1.37.28->boto3) (2.8.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.28->boto3) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install tqdm boto3 requests regex sentencepiece sacremoses datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YP3FxG9IF7O"
      },
      "source": [
        "DistilBERT pre-training 때 사용한 tokenizer를 불러온다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lGiZUoPby6e",
        "outputId": "aaa537fb-03d0-4e07-d330-a1c893ca20ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# DistilBERT 모델용 tokenizer 로드 (pretrained)\n",
        "# 이 tokenizer는 문장을 토큰화해서 모델이 이해할 수 있는 input_ids로 변환해줌\n",
        "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cvfl_uFLIMWO"
      },
      "source": [
        "뉴스 데이터로 Huggingface dataset의 fancyzhx/ag_news를 사용한다.\n",
        "AG News는 World, Sports, Science, Business, Sci/Tech 5개의 카테고리로 분류된다. label은 1부터 4까지의 숫자로 표현된다.\n",
        "\n",
        "truncation=False로 설정하여 텍스트를 자르지 않고, 입력 텍스트가 모델의 최대 길이를 초과하면 오류를 발생시킨다.\n",
        "모델의 최대 길이를 사용하기 위해 max_len을 설정하지 않는다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rE-y8sY9HuwP"
      },
      "outputs": [],
      "source": [
        "# ag_news 데이터셋의 5%만 로드 (학습 데이터와 테스트 데이터 각각)\n",
        "train_ds = load_dataset(\"fancyzhx/ag_news\", split=\"train[:5%]\")\n",
        "test_ds = load_dataset(\"fancyzhx/ag_news\", split=\"test[:5%]\")\n",
        "\n",
        "# 데이터를 배치로 묶기 위한 함수 정의\n",
        "def collate_fn(batch):\n",
        "    texts, labels = [], []  # 입력 문장들과 라벨들을 저장할 리스트\n",
        "\n",
        "    # 배치 내 각 샘플에 대해 text와 label 추출\n",
        "    for row in batch:\n",
        "        labels.append(row['label'])\n",
        "        texts.append(row['text'])\n",
        "\n",
        "    # tokenizer로 텍스트를 토큰화하고, 최대 길이로 패딩 및 자르기\n",
        "    # tokenizer는 사전에 정의되어 있어야 함 (예: tokenizer = AutoTokenizer.from_pretrained(...))\n",
        "    texts = torch.LongTensor(\n",
        "        tokenizer(texts, padding=True).input_ids\n",
        "    )\n",
        "\n",
        "    # 라벨 리스트를 LongTensor로 변환\n",
        "    labels = torch.LongTensor(labels)\n",
        "\n",
        "    # 모델 학습에 필요한 입력 (토큰화된 문장들)과 정답 라벨 반환\n",
        "    return texts, labels\n",
        "\n",
        "# 학습용 DataLoader 정의 (shuffle=True로 배치 순서 랜덤화)\n",
        "train_loader = DataLoader(\n",
        "    train_ds, batch_size=64, shuffle=True, collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# 테스트용 DataLoader 정의 (shuffle=False로 배치 순서 고정)\n",
        "test_loader = DataLoader(\n",
        "    test_ds, batch_size=64, shuffle=False, collate_fn=collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bF34XkoYIeEm"
      },
      "source": [
        "DistilBERT를 사용하기 위해 PyTorch hub에서 제공하는 DistilBERT를 불러온다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJaUp2Vob0U-",
        "outputId": "df674aa7-94b5-404f-9b24-83fc4852519e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertModel(\n",
              "  (embeddings): Embeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (transformer): Transformer(\n",
              "    (layer): ModuleList(\n",
              "      (0-5): 6 x TransformerBlock(\n",
              "        (attention): DistilBertSdpaAttention(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (ffn): FFN(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (activation): GELUActivation()\n",
              "        )\n",
              "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "#DistilBERT 모델을 PyTorch Hub에서 로드 후 model 출력\n",
        "model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'distilbert-base-uncased')\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh-tqY8WInQt"
      },
      "source": [
        "AG News는 4개의 카테고리를 가지고 있으므로 출력 차원을 4로 설정한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xW7ETZQzzNp2",
        "outputId": "b89ef2d8-66ef-494b-e0d3-9276f5ebbd3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
          ]
        }
      ],
      "source": [
        "from torch import nn\n",
        "\n",
        "# 텍스트 분류 모델 정의 (DistilBERT + Linear layer)\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # 사전학습된 DistilBERT 모델을 encoder로 불러옴 (pretrained transformer)\n",
        "        # 텍스트의 의미를 이해하고 벡터로 변환하는 역할을 한다\n",
        "        self.encoder = torch.hub.load('huggingface/pytorch-transformers', 'model', 'distilbert-base-uncased')\n",
        "\n",
        "        # [CLS] 토큰 분류기 정의\n",
        "        # 768차원의 입력을 받아 1차원으로 출력하는 선형 레이어 -> 뉴스 카테고리 출력\n",
        "        # 768은 DistilBERT의 hidden state 크기\n",
        "        self.classifier = nn.Linear(768, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encoder에 input_ids 전달\n",
        "        # DistilBERT는 여러 층의 트랜스포머 레이어로 구성되어 있다\n",
        "        # last_hidden_state는 마지막 트랜스포머 레이어의 출력값이다\n",
        "        # 이는 모델이 텍스트를 처리한 최종 문맥 표현(contextual representation)을 담고 있다\n",
        "        x = self.encoder(x)['last_hidden_state']\n",
        "\n",
        "        # [CLS] 토큰 위치 벡터를 classification head에 전달\n",
        "        x = self.classifier(x[:, 0])\n",
        "\n",
        "        # (batch_size, 4) 형태의 logits 반환\n",
        "        return x\n",
        "\n",
        "model = TextClassifier()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hFvSis0JLju"
      },
      "source": [
        "마지막 classifier layer를 제외한 나머지 부분을 freeze한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uyTciaPZ0KYo"
      },
      "outputs": [],
      "source": [
        "for param in model.encoder.parameters():\n",
        "  param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzXjPA3z4pgJ"
      },
      "source": [
        "분류 문제이므로 CrossEntropyLoss 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Foks5u95ZQ1_"
      },
      "outputs": [],
      "source": [
        "def accuracy(model, dataloader):\n",
        "    cnt = 0      # 전체 샘플 수\n",
        "    acc = 0      # 정답 개수 누적\n",
        "\n",
        "    for data in dataloader:\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "\n",
        "        preds = model(inputs)  # 로짓(logit) 출력\n",
        "\n",
        "        # argmax 사용하여 가장 높은 확률의 클래스 선택\n",
        "        preds = torch.argmax(preds, dim=-1)\n",
        "\n",
        "        cnt += labels.shape[0]  # 총 샘플 수 누적\n",
        "        acc += (labels == preds).sum().item()  # 예측이 맞은 수 누적\n",
        "\n",
        "    return acc / cnt  # 정확도 반환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvvaAEwCznt-",
        "outputId": "66332d87-5d12-4e36-daa9-5dd937f2e7e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   0 | Train Loss: 100.0052142739296\n",
            "=========> Train acc: 0.802 | Test acc: 0.813\n",
            "Epoch   1 | Train Loss: 69.63917607069016\n",
            "=========> Train acc: 0.834 | Test acc: 0.834\n",
            "Epoch   2 | Train Loss: 58.98968157172203\n",
            "=========> Train acc: 0.842 | Test acc: 0.847\n",
            "Epoch   3 | Train Loss: 54.392802745103836\n",
            "=========> Train acc: 0.838 | Test acc: 0.834\n",
            "Epoch   4 | Train Loss: 51.24429190158844\n",
            "=========> Train acc: 0.841 | Test acc: 0.839\n",
            "Epoch   5 | Train Loss: 49.12423440814018\n",
            "=========> Train acc: 0.846 | Test acc: 0.847\n",
            "Epoch   6 | Train Loss: 47.527686059474945\n",
            "=========> Train acc: 0.850 | Test acc: 0.855\n",
            "Epoch   7 | Train Loss: 46.82004718482494\n",
            "=========> Train acc: 0.843 | Test acc: 0.853\n",
            "Epoch   8 | Train Loss: 46.207649886608124\n",
            "=========> Train acc: 0.851 | Test acc: 0.853\n",
            "Epoch   9 | Train Loss: 44.412716165184975\n",
            "=========> Train acc: 0.855 | Test acc: 0.855\n"
          ]
        }
      ],
      "source": [
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 학습 설정\n",
        "lr = 0.001\n",
        "model = model.to('cuda')  # 모델을 GPU로 이동\n",
        "# 분류 문제이므로 CrossEntropyLoss 사용\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=lr)\n",
        "n_epochs = 10\n",
        "\n",
        "# 학습 루프\n",
        "for epoch in range(n_epochs):\n",
        "    total_loss = 0.\n",
        "    model.train()  # 학습 모드 설정\n",
        "\n",
        "    for data in train_loader:\n",
        "        model.zero_grad()  # 이전 gradient 초기화\n",
        "\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "\n",
        "        preds = model(inputs)\n",
        "\n",
        "        loss = loss_fn(preds, labels)  # 손실 계산\n",
        "        loss.backward()  # 역전파\n",
        "        optimizer.step()  # 파라미터 업데이트\n",
        "\n",
        "        total_loss += loss.item()  # loss 누적\n",
        "\n",
        "    print(f\"Epoch {epoch:3d} | Train Loss: {total_loss}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "      model.eval()  # 평가 모드로 전환 (계산 비활성화)\n",
        "      train_acc = accuracy(model, train_loader)\n",
        "      test_acc = accuracy(model, test_loader)\n",
        "\n",
        "      print(f\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_predictions(model, dataloader, num_samples):\n",
        "    model.eval()  # 평가 모드로 설정\n",
        "\n",
        "    # AG News 클래스 레이블\n",
        "    classes = {\n",
        "        0: \"World\",\n",
        "        1: \"Sports\",\n",
        "        2: \"Business\",\n",
        "        3: \"Sci/Tech\"\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # 데이터로더에서 배치 하나를 가져온다\n",
        "        texts, labels = next(iter(dataloader))\n",
        "        texts, labels = texts.to('cuda'), labels.to('cuda')\n",
        "\n",
        "        # 예측 수행\n",
        "        outputs = model(texts)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        # num_samples 개수만큼 결과 출력\n",
        "        print(\"\\n=== 예측 결과 ===\")\n",
        "        for idx in range(min(num_samples, len(texts))):\n",
        "            print(f\"\\n샘플 {idx+1}\")\n",
        "            print(f\"예측 클래스: {classes[predicted[idx].item()]}\")\n",
        "            print(f\"실제 클래스: {classes[labels[idx].item()]}\")\n",
        "            print(f\"예측 결과: {'정답' if predicted[idx] == labels[idx] else '오답'}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "# 예측 결과 확인\n",
        "with torch.no_grad():\n",
        "    train_acc = accuracy(model, train_loader)\n",
        "    test_acc = accuracy(model, test_loader)\n",
        "\n",
        "    print(f\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\")\n",
        "\n",
        "    visualize_predictions(model, test_loader, 10)"
      ],
      "metadata": {
        "id": "SdyQZzW-6nNp",
        "outputId": "924bf87a-09ed-4e03-9de4-14d999a44439",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========> Train acc: 0.854 | Test acc: 0.855\n",
            "\n",
            "=== 예측 결과 ===\n",
            "\n",
            "샘플 1\n",
            "예측 클래스: Business\n",
            "실제 클래스: Business\n",
            "예측 결과: 정답\n",
            "--------------------------------------------------\n",
            "\n",
            "샘플 2\n",
            "예측 클래스: Sci/Tech\n",
            "실제 클래스: Sci/Tech\n",
            "예측 결과: 정답\n",
            "--------------------------------------------------\n",
            "\n",
            "샘플 3\n",
            "예측 클래스: Sci/Tech\n",
            "실제 클래스: Sci/Tech\n",
            "예측 결과: 정답\n",
            "--------------------------------------------------\n",
            "\n",
            "샘플 4\n",
            "예측 클래스: Sci/Tech\n",
            "실제 클래스: Sci/Tech\n",
            "예측 결과: 정답\n",
            "--------------------------------------------------\n",
            "\n",
            "샘플 5\n",
            "예측 클래스: Sci/Tech\n",
            "실제 클래스: Sci/Tech\n",
            "예측 결과: 정답\n",
            "--------------------------------------------------\n",
            "\n",
            "샘플 6\n",
            "예측 클래스: Sci/Tech\n",
            "실제 클래스: Sci/Tech\n",
            "예측 결과: 정답\n",
            "--------------------------------------------------\n",
            "\n",
            "샘플 7\n",
            "예측 클래스: Sci/Tech\n",
            "실제 클래스: Sci/Tech\n",
            "예측 결과: 정답\n",
            "--------------------------------------------------\n",
            "\n",
            "샘플 8\n",
            "예측 클래스: Sci/Tech\n",
            "실제 클래스: Sci/Tech\n",
            "예측 결과: 정답\n",
            "--------------------------------------------------\n",
            "\n",
            "샘플 9\n",
            "예측 클래스: Sci/Tech\n",
            "실제 클래스: Sci/Tech\n",
            "예측 결과: 정답\n",
            "--------------------------------------------------\n",
            "\n",
            "샘플 10\n",
            "예측 클래스: Sci/Tech\n",
            "실제 클래스: Sci/Tech\n",
            "예측 결과: 정답\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}