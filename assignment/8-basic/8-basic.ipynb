{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pip install wandb datasets transformers trl torch peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb 설정\n",
    "wandb.init(project='Hanghae99-8-basic')\n",
    "wandb.run.name = 'gpt-finetuning-with-lora'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델과 토크나이저 로드\n",
    "model_name = \"facebook/opt-350m\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 로드\n",
    "dataset = load_dataset(\"sahil2801/CodeAlpaca-20k\", split=\"train\")\n",
    "\n",
    "print(\"=== 데이터셋 예시 ===\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 포매팅 함수 정의\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['instruction'])):\n",
    "        # input이 비어있는 경우 처리\n",
    "        input_text = example['input'][i].strip() if example['input'][i] else \"\"\n",
    "        \n",
    "        # 형식화된 프롬프트 생성\n",
    "        text = f\"[Instruction]\\n{example['instruction'][i].strip()}\\n\\n\"\n",
    "        if input_text:\n",
    "            text += f\"[Input]\\n{input_text}\\n\\n\"\n",
    "        text += f\"[Output]\\n{example['output'][i].strip()}\"\n",
    "        \n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "# 데이터 콜레이터 설정\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 lora_r 값으로 실험\n",
    "lora_r_values = [8, 128, 256]\n",
    "\n",
    "for lora_r in lora_r_values:\n",
    "    print(f\"\\n=== Training with LoRA rank {lora_r} ===\")\n",
    "    \n",
    "    # 기본 모델 로드\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    # LoRA 설정\n",
    "    lora_config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    \n",
    "    # LoRA 적용\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    # SFTTrainer 설정 및 학습\n",
    "    trainer = SFTTrainer(\n",
    "        model,\n",
    "        train_dataset=dataset,\n",
    "        args=SFTConfig(\n",
    "            output_dir=f\"/tmp/clm-instruction-tuning-lora-{lora_r}\",\n",
    "            max_seq_length=128\n",
    "        ),\n",
    "        formatting_func=formatting_prompts_func,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    # 학습 시작\n",
    "    trainer.train()\n",
    "    \n",
    "    # 메모리 사용량 출력\n",
    "    if torch.cuda.is_available():\n",
    "        print('Max Alloc:', round(torch.cuda.max_memory_allocated(0)/1024**3, 1), 'GB')\n",
    "    \n",
    "    # 모델 저장\n",
    "    trainer.save_model()\n",
    "    \n",
    "    # CUDA 캐시 정리\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
