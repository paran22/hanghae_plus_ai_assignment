{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymxatB5WYxlL"
      },
      "source": [
        "# [2주차] 심화과제: Multi-head Attention으로 감정 분석 모델 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1X7RM2du1zcr",
        "outputId": "0f64bd74-fd53-4b95-9526-690bf1533769"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.11/dist-packages (0.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacremoses) (2024.11.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install datasets sacremoses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOdhoBVA1zcu",
        "outputId": "f2d78f4b-47be-43bf-9675-74944d452b9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizerFast\n",
        "from tokenizers import (\n",
        "    decoders,\n",
        "    models,\n",
        "    normalizers,\n",
        "    pre_tokenizers,\n",
        "    processors,\n",
        "    trainers,\n",
        "    Tokenizer,\n",
        ")\n",
        "\n",
        "\n",
        "train_ds = load_dataset(\"stanfordnlp/imdb\", split=\"train\")\n",
        "test_ds = load_dataset(\"stanfordnlp/imdb\", split=\"test\")\n",
        "\n",
        "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "  max_len = 400\n",
        "  texts, labels = [], []\n",
        "  for row in batch:\n",
        "    labels.append(row['label']) # 레이블 추가 (긍정/부정)\n",
        "    texts.append(row['text'])  # 텍스트 추가 (영화 리뷰)\n",
        "\n",
        "  texts = torch.LongTensor(tokenizer(texts, padding=True, truncation=True, max_length=max_len).input_ids)\n",
        "  labels = torch.LongTensor(labels)\n",
        "\n",
        "  return texts, labels\n",
        "\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds, batch_size=64, shuffle=True, collate_fn=collate_fn\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_ds, batch_size=64, shuffle=False, collate_fn=collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-FshZcTZBQ2"
      },
      "source": [
        "## Multi-head Attention\n",
        "\n",
        "Multi-head Attention은 Transformer 모델의 핵심 구성 요소 중 하나로, 여러 개의 어텐션 헤드를 사용하여 입력 시퀀스의 다양한 부분 간의 상호작용을 동시에 학습할 수 있게 한다.\n",
        "각 어텐션 헤드는 입력 시퀀스를 독립적으로 처리하여 서로 다른 표현 공간에서의 상호작용을 학습한다.\n",
        "이를 통해 모델은 입력 시퀀스의 다양한 부분 간의 복잡한 관계를 더 잘 이해할 수 있게 된다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MBlMVMZcRAxv"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from math import sqrt\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, n_heads):\n",
        "        super().__init__()\n",
        "\n",
        "        # d_model이 n_heads로 나누어 떨어지는지 확인\n",
        "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = d_model // n_heads  # D'\n",
        "\n",
        "        self.wq = nn.Linear(input_dim, d_model)\n",
        "        self.wk = nn.Linear(input_dim, d_model)\n",
        "        self.wv = nn.Linear(input_dim, d_model)\n",
        "        # multi head attention의 마지막 단계에서 여러 헤드에서 나온 정보를\n",
        "        # 다시 원래의 차원으로 투영하는 역할을 한다.\n",
        "        self.dense = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        batch_size = x.size(0)\n",
        "        seq_length = x.size(1)\n",
        "\n",
        "        # 1. Q, K, V 생성 (기존과 동일)\n",
        "        q, k, v = self.wq(x), self.wk(x), self.wv(x)  # (B, S, D)\n",
        "\n",
        "        # 2. Q, K, V를 head 수만큼 분할하여 reshape\n",
        "        q = q.view(batch_size, seq_length, self.n_heads, self.d_head)  # (B, S, H, D')\n",
        "        k = k.view(batch_size, seq_length, self.n_heads, self.d_head)\n",
        "        v = v.view(batch_size, seq_length, self.n_heads, self.d_head)\n",
        "\n",
        "        # 3. Transpose하여 head 차원을 앞으로\n",
        "        q = q.transpose(1, 2)  # (B, H, S, D')\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        # 4. Attention Score 계산\n",
        "        score = torch.matmul(q, k.transpose(-1, -2))  # (B, H, S, S)\n",
        "        score = score / sqrt(self.d_head)  # D'로 나누기\n",
        "\n",
        "        # 5. Mask 적용 (mask 차원 확장)\n",
        "        if mask is not None:\n",
        "            # mask: (B, 1, S) -> (B, 1, 1, S)\n",
        "            mask = mask.unsqueeze(1)\n",
        "            score = score + (mask * -1e9)\n",
        "\n",
        "        # 6. Attention 계산 및 원래 shape으로 복원\n",
        "        score = self.softmax(score)\n",
        "        result = torch.matmul(score, v)  # (B, H, S, D')\n",
        "\n",
        "        # Transpose 및 Reshape\n",
        "        result = result.transpose(1, 2)  # (B, S, H, D')\n",
        "        result = result.contiguous().view(batch_size, seq_length, self.d_model)  # (B, S, D)\n",
        "\n",
        "        # 7. Output projection\n",
        "        result = self.dense(result)\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VZHPCn9AS5Gp"
      },
      "outputs": [],
      "source": [
        "class TransformerLayer(nn.Module):\n",
        "  def __init__(self, input_dim, d_model, dff, n_heads, dropout_rate):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_dim = input_dim\n",
        "    self.d_model = d_model\n",
        "    self.dff = dff\n",
        "    self.n_heads = n_heads\n",
        "\n",
        "    # SelfAttention을 MultiHeadAttention으로 변경\n",
        "    self.mha = MultiHeadAttention(input_dim, d_model, n_heads)\n",
        "\n",
        "    # FFN은 그대로 유지\n",
        "    self.ffn = nn.Sequential(\n",
        "        nn.Linear(d_model, dff),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(dff, d_model)\n",
        "    )\n",
        "\n",
        "    # Layer Normalization\n",
        "    self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "    self.layer_norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    # Dropout\n",
        "    self.dropout1 = nn.Dropout(dropout_rate)\n",
        "    self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    # Multi-Head Attention with residual connection and layer norm\n",
        "    attn_output = self.mha(x, mask)\n",
        "    attn_output = self.dropout1(attn_output)\n",
        "    out1 = self.layer_norm1(attn_output + x)\n",
        "\n",
        "    # Feed Forward with residual connection and layer norm\n",
        "    ffn_output = self.ffn(out1)\n",
        "    ffn_output = self.dropout2(ffn_output)\n",
        "    out2 = self.layer_norm2(ffn_output + out1)\n",
        "\n",
        "    return out2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3VYrqTJagS1"
      },
      "source": [
        "## Positional encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uf_jMQWDUR79",
        "outputId": "ceec0281-aa2b-474e-c874-54a9cbb64868"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 400, 256])\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, None], np.arange(d_model)[None, :], d_model)\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[None, ...]\n",
        "\n",
        "    return torch.FloatTensor(pos_encoding)\n",
        "\n",
        "\n",
        "max_len = 400\n",
        "print(positional_encoding(max_len, 256).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8MaiCGh8TsDH"
      },
      "outputs": [],
      "source": [
        "class TextClassifier(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, n_layers, dff, n_heads, dropout_rate):\n",
        "    super().__init__()\n",
        "\n",
        "    self.vocab_size = vocab_size\n",
        "    self.d_model = d_model\n",
        "    self.n_layers = n_layers\n",
        "    self.dff = dff\n",
        "    self.n_heads = n_heads\n",
        "    self.dropout_rate = dropout_rate\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = nn.parameter.Parameter(positional_encoding(max_len, d_model), requires_grad=False)\n",
        "    self.layers = nn.ModuleList([TransformerLayer(d_model, d_model, dff, n_heads, dropout_rate) for _ in range(n_layers)])\n",
        "    self.classification = nn.Linear(d_model, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    mask = (x == tokenizer.pad_token_id)\n",
        "    mask = mask[:, None, :]\n",
        "    seq_len = x.shape[1]\n",
        "\n",
        "    x = self.embedding(x)\n",
        "    x = x * sqrt(self.d_model)\n",
        "    x = x + self.pos_encoding[:, :seq_len]\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "\n",
        "    x = x[:, 0]\n",
        "    x = self.classification(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "model = TextClassifier(len(tokenizer), 32, 4, 32, 4, 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDq05OlAb2lB"
      },
      "source": [
        "## 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YHVVsWBPQmnv"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "lr = 0.001\n",
        "model = model.to(device)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "r88BALxO1zc1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def accuracy(model, dataloader):\n",
        "  cnt = 0\n",
        "  acc = 0\n",
        "\n",
        "  for data in dataloader:\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "    preds = model(inputs)\n",
        "    preds = (preds > 0).long()[..., 0]\n",
        "\n",
        "    cnt += labels.shape[0]\n",
        "    acc += (labels == preds).sum().item()\n",
        "\n",
        "  return acc / cnt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "al_b56TYRILq",
        "outputId": "eb22cf36-de24-4c34-c5fb-4e90c7f14ea2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   0 | Train Loss: 209.111279129982\n",
            "=========> Train acc: 0.825 | Test acc: 0.793\n",
            "Epoch   1 | Train Loss: 148.36042469739914\n",
            "=========> Train acc: 0.881 | Test acc: 0.819\n",
            "Epoch   2 | Train Loss: 118.54504172503948\n",
            "=========> Train acc: 0.907 | Test acc: 0.818\n",
            "Epoch   3 | Train Loss: 95.20489977300167\n",
            "=========> Train acc: 0.941 | Test acc: 0.826\n",
            "Epoch   4 | Train Loss: 74.2625348046422\n",
            "=========> Train acc: 0.961 | Test acc: 0.828\n",
            "Epoch   5 | Train Loss: 55.13052459806204\n",
            "=========> Train acc: 0.976 | Test acc: 0.822\n",
            "Epoch   6 | Train Loss: 41.68866502121091\n",
            "=========> Train acc: 0.985 | Test acc: 0.823\n",
            "Epoch   7 | Train Loss: 31.84616607055068\n",
            "=========> Train acc: 0.984 | Test acc: 0.824\n",
            "Epoch   8 | Train Loss: 26.017943282146007\n",
            "=========> Train acc: 0.990 | Test acc: 0.819\n",
            "Epoch   9 | Train Loss: 22.650948900263757\n",
            "=========> Train acc: 0.991 | Test acc: 0.823\n",
            "Epoch  10 | Train Loss: 20.734021531883627\n",
            "=========> Train acc: 0.992 | Test acc: 0.823\n",
            "Epoch  11 | Train Loss: 18.44123435812071\n",
            "=========> Train acc: 0.990 | Test acc: 0.818\n",
            "Epoch  12 | Train Loss: 17.392526387702674\n",
            "=========> Train acc: 0.993 | Test acc: 0.824\n",
            "Epoch  13 | Train Loss: 16.88445278443396\n",
            "=========> Train acc: 0.993 | Test acc: 0.820\n",
            "Epoch  14 | Train Loss: 15.766353387152776\n",
            "=========> Train acc: 0.989 | Test acc: 0.821\n",
            "Epoch  15 | Train Loss: 15.028951856307685\n",
            "=========> Train acc: 0.995 | Test acc: 0.823\n",
            "Epoch  16 | Train Loss: 13.238352596526965\n",
            "=========> Train acc: 0.994 | Test acc: 0.825\n",
            "Epoch  17 | Train Loss: 13.79516678955406\n",
            "=========> Train acc: 0.996 | Test acc: 0.822\n",
            "Epoch  18 | Train Loss: 14.393958314787596\n",
            "=========> Train acc: 0.993 | Test acc: 0.823\n",
            "Epoch  19 | Train Loss: 12.801313441479579\n",
            "=========> Train acc: 0.996 | Test acc: 0.823\n",
            "Epoch  20 | Train Loss: 10.297449073987082\n",
            "=========> Train acc: 0.996 | Test acc: 0.823\n",
            "Epoch  21 | Train Loss: 13.364861776586622\n",
            "=========> Train acc: 0.995 | Test acc: 0.826\n",
            "Epoch  22 | Train Loss: 12.540673050098121\n",
            "=========> Train acc: 0.995 | Test acc: 0.821\n",
            "Epoch  23 | Train Loss: 11.752728551393375\n",
            "=========> Train acc: 0.995 | Test acc: 0.823\n",
            "Epoch  24 | Train Loss: 11.677318108035251\n",
            "=========> Train acc: 0.994 | Test acc: 0.818\n",
            "Epoch  25 | Train Loss: 11.857613605330698\n",
            "=========> Train acc: 0.994 | Test acc: 0.819\n",
            "Epoch  26 | Train Loss: 10.54650110239163\n",
            "=========> Train acc: 0.996 | Test acc: 0.823\n",
            "Epoch  27 | Train Loss: 11.226504799211398\n",
            "=========> Train acc: 0.991 | Test acc: 0.821\n",
            "Epoch  28 | Train Loss: 10.168243586667813\n",
            "=========> Train acc: 0.990 | Test acc: 0.821\n",
            "Epoch  29 | Train Loss: 9.44462702376768\n",
            "=========> Train acc: 0.996 | Test acc: 0.823\n",
            "Epoch  30 | Train Loss: 10.691482422989793\n",
            "=========> Train acc: 0.994 | Test acc: 0.819\n",
            "Epoch  31 | Train Loss: 10.098800031701103\n",
            "=========> Train acc: 0.996 | Test acc: 0.826\n",
            "Epoch  32 | Train Loss: 9.370537240523845\n",
            "=========> Train acc: 0.997 | Test acc: 0.825\n",
            "Epoch  33 | Train Loss: 9.676853698445484\n",
            "=========> Train acc: 0.995 | Test acc: 0.828\n",
            "Epoch  34 | Train Loss: 9.843731152359396\n",
            "=========> Train acc: 0.996 | Test acc: 0.826\n",
            "Epoch  35 | Train Loss: 8.54264633380808\n",
            "=========> Train acc: 0.995 | Test acc: 0.817\n",
            "Epoch  36 | Train Loss: 8.211690241238102\n",
            "=========> Train acc: 0.997 | Test acc: 0.825\n",
            "Epoch  37 | Train Loss: 7.350855450727977\n",
            "=========> Train acc: 0.997 | Test acc: 0.825\n",
            "Epoch  38 | Train Loss: 8.949879608408082\n",
            "=========> Train acc: 0.997 | Test acc: 0.824\n",
            "Epoch  39 | Train Loss: 9.975371263804846\n",
            "=========> Train acc: 0.996 | Test acc: 0.824\n",
            "Epoch  40 | Train Loss: 8.837506013282109\n",
            "=========> Train acc: 0.996 | Test acc: 0.820\n",
            "Epoch  41 | Train Loss: 8.304509443230927\n",
            "=========> Train acc: 0.997 | Test acc: 0.823\n",
            "Epoch  42 | Train Loss: 6.616424378356896\n",
            "=========> Train acc: 0.996 | Test acc: 0.823\n",
            "Epoch  43 | Train Loss: 7.8390449487487786\n",
            "=========> Train acc: 0.996 | Test acc: 0.822\n",
            "Epoch  44 | Train Loss: 7.782533646211959\n",
            "=========> Train acc: 0.990 | Test acc: 0.807\n",
            "Epoch  45 | Train Loss: 8.897775574354455\n",
            "=========> Train acc: 0.997 | Test acc: 0.824\n",
            "Epoch  46 | Train Loss: 6.685146043426357\n",
            "=========> Train acc: 0.997 | Test acc: 0.822\n",
            "Epoch  47 | Train Loss: 6.729977974406211\n",
            "=========> Train acc: 0.997 | Test acc: 0.817\n",
            "Epoch  48 | Train Loss: 7.577435964776669\n",
            "=========> Train acc: 0.997 | Test acc: 0.827\n",
            "Epoch  49 | Train Loss: 7.711191213107668\n",
            "=========> Train acc: 0.997 | Test acc: 0.824\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 20\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  total_loss = 0.\n",
        "  model.train()\n",
        "  for data in train_loader:\n",
        "    model.zero_grad()\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to(device), labels.to(device).float()\n",
        "\n",
        "    preds = model(inputs)[..., 0]\n",
        "    loss = loss_fn(preds, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  print(f\"Epoch {epoch:3d} | Train Loss: {total_loss}\")\n",
        "\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    train_acc = accuracy(model, train_loader)\n",
        "    test_acc = accuracy(model, test_loader)\n",
        "    print(f\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (welcomedl)",
      "language": "python",
      "name": "welcomedl"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}