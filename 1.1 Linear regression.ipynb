{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dS_t0-ik_WC1"
   },
   "source": [
    "# Linear Regression 실습\n",
    "\n",
    "이번 실습에서는 linear regression에 대한 gradient descent를 직접 구현해봅니다. 여기서 사용할 문제들은 크게 두 가지로 OR 문제와 XOR 문제입니다.\n",
    "\n",
    "먼저 필요한 library들을 import합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1723361278387,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "DEJFJkL6qHB9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cG2fJsOF8LsP"
   },
   "source": [
    "## OR Problem\n",
    "\n",
    "OR은 0 또는 1의 값을 가질 수 있는 두 개의 정수를 입력으로 받아 둘 중에 하나라도 1이면 1을 출력하고 아니면 0을 출력하는 문제입니다.\n",
    "즉, 우리가 학습하고자 하는 함수는 2개의 정수를 입력받아 하나의 정수를 출력하면됩니다. 이러한 함수를 학습하기 위한 data는 다음과 같이 구성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1723361278864,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "SsEdD6T7qLJH",
    "outputId": "a8c91b61-98b4-45af-be7b-865abe2c8b1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [0., 0.],\n",
    "    [0., 1.],\n",
    "    [1., 0.],\n",
    "    [1., 1.]\n",
    "])\n",
    "y = torch.tensor([0, 1, 1, 1])\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YyD1n6wf_3ey"
   },
   "source": [
    "출력 결과에서 볼 수 있다시피 $x$의 shape은 (4, 2)로, 총 4개의 two-dimensional data 임을 알 수 있습니다. $y$는 각 $x_i$에 대한 label로 우리가 설정한 문제의 조건을 잘 따라가는 것을 알 수 있습니다.\n",
    "\n",
    "다음으로는 linear regression의 parameter들인 $w, b$를 정의하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 423,
     "status": "ok",
     "timestamp": 1723361279282,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "uzG4w1VYqlhz",
    "outputId": "8c5ad5c5-dea3-4b59-cebe-feeca50507f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2]) torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "w = torch.randn((1, 2))\n",
    "b = torch.randn((1, 1))\n",
    "\n",
    "print(w.shape, b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELTb9Dl-AYbp"
   },
   "source": [
    "$w$는 1x2의 벡터이고 $b$는 1x1의 scalar임을 알 수 있습니다. 여기서는 `torch.randn`을 사용하여 standard normal distribution을 가지고 초기화하였습니다.\n",
    "\n",
    "이러한 $w, b$와 data $x, y$가 주어졌을 때 우리가 학습한 $w, b$의 성능을 평가하는 함수를 구현합시다.\n",
    "평가 함수는 다음과 같이 MSE로 정의됩니다:\n",
    "$$l(f) := MSE(f(x), y) = \\frac{1}{n} \\sum_{i=1}^n (f(x_i) - y)^2.$$\n",
    "이를 구현한 코드는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1723361279282,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "LBxldV7D8UMf"
   },
   "outputs": [],
   "source": [
    "def pred(w, b, x):\n",
    "  return torch.matmul(w, x.T) + b\n",
    "\n",
    "\n",
    "def loss(w, b, x, y):\n",
    "  return (y - pred(w, b, x)).pow(2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmM79Ly6VyBw"
   },
   "source": [
    "먼저 `def pred(w, b, x)`는 $wx^T + b$, 즉 1차 함수 $f$의 $x$에 대한 결과를 반환하는 함수를 구현했습니다.\n",
    "이를 이용하여 주어진 label $y$와의 MSE를 측정하는 코드가 `def loss(w, b, x, y)`에 구현되어있습니다.\n",
    "\n",
    "다음은 MSE를 기반으로 $w, b$의 gradient를 구하는 코드를 구현하겠습니다.\n",
    "MSE에 대한 $w$의 gradient는 다음과 같이 구할 수 있습니다:\n",
    "$$\\frac{\\partial l}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^n 2(wx_i^T + b - y)x_i.$$\n",
    "$b$에 대한 gradient는 다음과 같습니다:\n",
    "$$\\frac{\\partial l}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^n 2(wx_i^T + b - y).$$\n",
    "이를 코드로 구현하면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1723361279282,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "rLrsXZ0iq13m"
   },
   "outputs": [],
   "source": [
    "def grad_w(w, b, x, y):\n",
    "  # w: (1, d), b: (1, 1), x: (n, d), y: (n)\n",
    "  tmp1 = torch.matmul(w, x.T)  # (1, n)\n",
    "  tmp2 = tmp1 + b              # (1, n)\n",
    "  tmp3 = 2 * (tmp2 - y[None])  # (1, n)\n",
    "  grad_item = tmp3.T * x       # (n, d)\n",
    "  return grad_item.mean(dim=0, keepdim=True)  # (1, d)\n",
    "\n",
    "\n",
    "def grad_b(w, b, x, y):\n",
    "  # w: (1, d), b: (1, 1), x: (n, d), y: (n)\n",
    "  grad_item = 2 * (torch.matmul(w, x.T) + b - y[None])  # (1, n)\n",
    "  return grad_item.mean(dim=-1, keepdim=True)           # (1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCbBU1RaX6O5"
   },
   "source": [
    "여기서 중요한 것은 shape에 맞춰서 연산을 잘 사용해야한다는 것입니다. Shape과 관련된 설명은 `[Chapter 0]`의 Numpy에서 설명했으니, 복습하신다는 느낌으로 주석으로 써놓은 shape들을 유도해보시면 좋을 것 같습니다. 중요한 것은 반환되는 tensor의 shape이 우리가 구하고자 하는 gradient와 일치해야 한다는 것입니다. 예를 들어 $w$의 $l$에 대한 gradient는 $w$와 shape이 동일해야 합니다.\n",
    "\n",
    "마지막으로 gradient descent 함수를 구현하겠습니다. Gradient descent는 다음과 같이 정의됩니다:\n",
    "$$w^{(new)} = w^{(old)} - \\eta \\frac{\\partial l}{\\partial w} \\biggr\\rvert_{w = w^{(old)}}.$$\n",
    "Gradient는 위에서 구현했으니 이를 활용하여 learning rate $\\eta$가 주어졌을 때 $w, b$를 update하는 코드를 구현할 수 있습니다. 구현한 결과는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1723361279282,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "wFRS72UF8QVv"
   },
   "outputs": [],
   "source": [
    "def update(x, y, w, b, lr):\n",
    "  w = w - lr * grad_w(w, b, x, y)\n",
    "  b = b - lr * grad_b(w, b, x, y)\n",
    "  return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b93uvneVZ7bF"
   },
   "source": [
    "Gradient descent에 해당하는 코드는 모두 구현하였습니다. 이제 학습하는 코드를 구현하겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1723361279283,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "Pa6fA_ZUFI-0"
   },
   "outputs": [],
   "source": [
    "def train(n_epochs, lr, w, b, x, y):\n",
    "  for e in range(n_epochs):\n",
    "    w, b = update(x, y, w, b, lr)\n",
    "    print(f\"Epoch {e:3d} | Loss: {loss(w, b, x, y)}\")\n",
    "  return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrJGKWilaBFq"
   },
   "source": [
    "여기서 `n_epochs`는 update를 하는 횟수를 의미합니다. 매 update 이후에 `loss` 함수를 사용하여 잘 수렴하고 있는지 살펴봅니다. 실제로 이 함수를 실행한 결과는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1723361279283,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "zFk-josgBSj7",
    "outputId": "ec127ca9-a563-43ac-8adc-8e1e398ecb5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Loss: 1.9151513576507568\n",
      "Epoch   1 | Loss: 1.0231670141220093\n",
      "Epoch   2 | Loss: 0.5765958428382874\n",
      "Epoch   3 | Loss: 0.35151296854019165\n",
      "Epoch   4 | Loss: 0.23677843809127808\n",
      "Epoch   5 | Loss: 0.17718347907066345\n",
      "Epoch   6 | Loss: 0.1452697217464447\n",
      "Epoch   7 | Loss: 0.1273564100265503\n",
      "Epoch   8 | Loss: 0.11660972237586975\n",
      "Epoch   9 | Loss: 0.10960286110639572\n",
      "Epoch  10 | Loss: 0.10460754483938217\n",
      "Epoch  11 | Loss: 0.10074438154697418\n",
      "Epoch  12 | Loss: 0.09755989164113998\n",
      "Epoch  13 | Loss: 0.09481535106897354\n",
      "Epoch  14 | Loss: 0.09238116443157196\n",
      "Epoch  15 | Loss: 0.090183787047863\n",
      "Epoch  16 | Loss: 0.08817896246910095\n",
      "Epoch  17 | Loss: 0.08633802086114883\n",
      "Epoch  18 | Loss: 0.08464096486568451\n",
      "Epoch  19 | Loss: 0.08307266235351562\n",
      "Epoch  20 | Loss: 0.08162106573581696\n",
      "Epoch  21 | Loss: 0.08027605712413788\n",
      "Epoch  22 | Loss: 0.0790288895368576\n",
      "Epoch  23 | Loss: 0.07787180691957474\n",
      "Epoch  24 | Loss: 0.07679787278175354\n",
      "Epoch  25 | Loss: 0.07580074667930603\n",
      "Epoch  26 | Loss: 0.07487472146749496\n",
      "Epoch  27 | Loss: 0.07401451468467712\n",
      "Epoch  28 | Loss: 0.07321528345346451\n",
      "Epoch  29 | Loss: 0.07247257232666016\n",
      "Epoch  30 | Loss: 0.07178227603435516\n",
      "Epoch  31 | Loss: 0.0711405947804451\n",
      "Epoch  32 | Loss: 0.07054401189088821\n",
      "Epoch  33 | Loss: 0.06998929381370544\n",
      "Epoch  34 | Loss: 0.06947343051433563\n",
      "Epoch  35 | Loss: 0.06899365037679672\n",
      "Epoch  36 | Loss: 0.06854736804962158\n",
      "Epoch  37 | Loss: 0.0681321918964386\n",
      "Epoch  38 | Loss: 0.06774593144655228\n",
      "Epoch  39 | Loss: 0.06738650798797607\n",
      "Epoch  40 | Loss: 0.06705203652381897\n",
      "Epoch  41 | Loss: 0.06674077361822128\n",
      "Epoch  42 | Loss: 0.0664510428905487\n",
      "Epoch  43 | Loss: 0.06618134677410126\n",
      "Epoch  44 | Loss: 0.06593029201030731\n",
      "Epoch  45 | Loss: 0.06569655239582062\n",
      "Epoch  46 | Loss: 0.06547890603542328\n",
      "Epoch  47 | Loss: 0.06527624279260635\n",
      "Epoch  48 | Loss: 0.06508752703666687\n",
      "Epoch  49 | Loss: 0.06491176038980484\n",
      "Epoch  50 | Loss: 0.06474804878234863\n",
      "Epoch  51 | Loss: 0.06459555774927139\n",
      "Epoch  52 | Loss: 0.06445351243019104\n",
      "Epoch  53 | Loss: 0.06432119756937027\n",
      "Epoch  54 | Loss: 0.06419789791107178\n",
      "Epoch  55 | Loss: 0.0640830248594284\n",
      "Epoch  56 | Loss: 0.0639759972691536\n",
      "Epoch  57 | Loss: 0.06387625634670258\n",
      "Epoch  58 | Loss: 0.06378331780433655\n",
      "Epoch  59 | Loss: 0.06369670480489731\n",
      "Epoch  60 | Loss: 0.06361596286296844\n",
      "Epoch  61 | Loss: 0.06354072690010071\n",
      "Epoch  62 | Loss: 0.06347058713436127\n",
      "Epoch  63 | Loss: 0.06340521574020386\n",
      "Epoch  64 | Loss: 0.06334426999092102\n",
      "Epoch  65 | Loss: 0.06328745186328888\n",
      "Epoch  66 | Loss: 0.06323447078466415\n",
      "Epoch  67 | Loss: 0.06318508833646774\n",
      "Epoch  68 | Loss: 0.06313904374837875\n",
      "Epoch  69 | Loss: 0.06309610605239868\n",
      "Epoch  70 | Loss: 0.063056081533432\n",
      "Epoch  71 | Loss: 0.06301874667406082\n",
      "Epoch  72 | Loss: 0.06298395246267319\n",
      "Epoch  73 | Loss: 0.06295149028301239\n",
      "Epoch  74 | Loss: 0.06292121112346649\n",
      "Epoch  75 | Loss: 0.06289296597242355\n",
      "Epoch  76 | Loss: 0.06286663562059402\n",
      "Epoch  77 | Loss: 0.06284207850694656\n",
      "Epoch  78 | Loss: 0.06281916052103043\n",
      "Epoch  79 | Loss: 0.06279779225587845\n",
      "Epoch  80 | Loss: 0.06277784705162048\n",
      "Epoch  81 | Loss: 0.06275927275419235\n",
      "Epoch  82 | Loss: 0.06274192035198212\n",
      "Epoch  83 | Loss: 0.0627257451415062\n",
      "Epoch  84 | Loss: 0.06271065771579742\n",
      "Epoch  85 | Loss: 0.06269656121730804\n",
      "Epoch  86 | Loss: 0.06268342584371567\n",
      "Epoch  87 | Loss: 0.06267116963863373\n",
      "Epoch  88 | Loss: 0.06265974044799805\n",
      "Epoch  89 | Loss: 0.06264907121658325\n",
      "Epoch  90 | Loss: 0.06263910979032516\n",
      "Epoch  91 | Loss: 0.0626298263669014\n",
      "Epoch  92 | Loss: 0.06262116134166718\n",
      "Epoch  93 | Loss: 0.06261306256055832\n",
      "Epoch  94 | Loss: 0.06260553002357483\n",
      "Epoch  95 | Loss: 0.06259850412607193\n",
      "Epoch  96 | Loss: 0.06259191036224365\n",
      "Epoch  97 | Loss: 0.06258578598499298\n",
      "Epoch  98 | Loss: 0.06258007138967514\n",
      "Epoch  99 | Loss: 0.06257472932338715\n",
      "tensor([[0.4870, 0.4893]]) tensor([[0.2638]])\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "lr = 0.1\n",
    "\n",
    "w, b = train(n_epochs, lr, w, b, x, y)\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2Ny-YkAaNh8"
   },
   "source": [
    "잘 수렴하는 것을 확인하였습니다. 마지막으로 OR data에 대한 $w, b$의 예측 결과와 label을 비교해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1723361279283,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "IggGP969Bh-w",
    "outputId": "c9a5ccb0-5fdc-4f86-ed30-157f56f97d04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2638, 0.7531, 0.7508, 1.2401]])\n",
      "tensor([0, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(pred(w, b, x))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8gKvx2naWDP"
   },
   "source": [
    "예측 결과를 볼 수 있다시피 우리의 linear regression model은 0과 1에 해당하는 data를 잘 구분하는 것을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMXZLfd3DC50"
   },
   "source": [
    "# XOR Problem\n",
    "\n",
    "이번에는 XOR를 학습해보겠습니다. XOR은 OR과 똑같은 입력을 받는 문제로, 두 개의 0 또는 1의 정수가 들어왔을 때 두 정수가 다르면 1, 아니면 0을 출력해야 합니다.\n",
    "먼저 data를 만들어보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1723361279283,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "JtFGsqNXCjtM",
    "outputId": "7183941e-8298-4f2d-f443-9c20f39aec11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [0., 0.],\n",
    "    [0., 1.],\n",
    "    [1., 0.],\n",
    "    [1., 1.]\n",
    "])\n",
    "y = torch.tensor([0, 1, 1, 0])\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYRtKaviaedO"
   },
   "source": [
    "보시다시피 shape이나 생성 과정은 OR과 똑같습니다. 다른 것은 $y$에서의 labeling입니다. OR과 다르게 $x = (1, 1)$에 대해서는 0을 labeling했습니다.\n",
    "이러한 사소한 차이에 대해서도 linear regression model이 잘 학습할 수 있을지 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1723361279283,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "iw5UUqKdDG98",
    "outputId": "b37eb7e7-ff7f-4887-e432-c5f597179f78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Loss: 0.25006699562072754\n",
      "Epoch   1 | Loss: 0.25006258487701416\n",
      "Epoch   2 | Loss: 0.2500584125518799\n",
      "Epoch   3 | Loss: 0.2500545084476471\n",
      "Epoch   4 | Loss: 0.2500509023666382\n",
      "Epoch   5 | Loss: 0.250047504901886\n",
      "Epoch   6 | Loss: 0.2500443756580353\n",
      "Epoch   7 | Loss: 0.2500414252281189\n",
      "Epoch   8 | Loss: 0.25003868341445923\n",
      "Epoch   9 | Loss: 0.2500361204147339\n",
      "Epoch  10 | Loss: 0.25003373622894287\n",
      "Epoch  11 | Loss: 0.2500314712524414\n",
      "Epoch  12 | Loss: 0.25002941489219666\n",
      "Epoch  13 | Loss: 0.2500274181365967\n",
      "Epoch  14 | Loss: 0.2500256299972534\n",
      "Epoch  15 | Loss: 0.25002390146255493\n",
      "Epoch  16 | Loss: 0.2500223219394684\n",
      "Epoch  17 | Loss: 0.2500208616256714\n",
      "Epoch  18 | Loss: 0.25001946091651917\n",
      "Epoch  19 | Loss: 0.2500181794166565\n",
      "Epoch  20 | Loss: 0.2500169575214386\n",
      "Epoch  21 | Loss: 0.25001585483551025\n",
      "Epoch  22 | Loss: 0.2500147819519043\n",
      "Epoch  23 | Loss: 0.2500137984752655\n",
      "Epoch  24 | Loss: 0.2500128746032715\n",
      "Epoch  25 | Loss: 0.25001204013824463\n",
      "Epoch  26 | Loss: 0.25001126527786255\n",
      "Epoch  27 | Loss: 0.25001049041748047\n",
      "Epoch  28 | Loss: 0.25000977516174316\n",
      "Epoch  29 | Loss: 0.25000911951065063\n",
      "Epoch  30 | Loss: 0.2500085234642029\n",
      "Epoch  31 | Loss: 0.2500079870223999\n",
      "Epoch  32 | Loss: 0.2500074505805969\n",
      "Epoch  33 | Loss: 0.2500069737434387\n",
      "Epoch  34 | Loss: 0.2500064969062805\n",
      "Epoch  35 | Loss: 0.2500060498714447\n",
      "Epoch  36 | Loss: 0.25000566244125366\n",
      "Epoch  37 | Loss: 0.250005304813385\n",
      "Epoch  38 | Loss: 0.25000491738319397\n",
      "Epoch  39 | Loss: 0.2500046193599701\n",
      "Epoch  40 | Loss: 0.25000429153442383\n",
      "Epoch  41 | Loss: 0.25000399351119995\n",
      "Epoch  42 | Loss: 0.25000372529029846\n",
      "Epoch  43 | Loss: 0.25000351667404175\n",
      "Epoch  44 | Loss: 0.25000324845314026\n",
      "Epoch  45 | Loss: 0.25000303983688354\n",
      "Epoch  46 | Loss: 0.2500028610229492\n",
      "Epoch  47 | Loss: 0.2500026226043701\n",
      "Epoch  48 | Loss: 0.2500024735927582\n",
      "Epoch  49 | Loss: 0.25000232458114624\n",
      "Epoch  50 | Loss: 0.2500021755695343\n",
      "Epoch  51 | Loss: 0.25000202655792236\n",
      "Epoch  52 | Loss: 0.2500019073486328\n",
      "Epoch  53 | Loss: 0.2500017583370209\n",
      "Epoch  54 | Loss: 0.2500016391277313\n",
      "Epoch  55 | Loss: 0.25000154972076416\n",
      "Epoch  56 | Loss: 0.2500014305114746\n",
      "Epoch  57 | Loss: 0.25000131130218506\n",
      "Epoch  58 | Loss: 0.2500012516975403\n",
      "Epoch  59 | Loss: 0.2500011920928955\n",
      "Epoch  60 | Loss: 0.25000110268592834\n",
      "Epoch  61 | Loss: 0.2500010132789612\n",
      "Epoch  62 | Loss: 0.2500009536743164\n",
      "Epoch  63 | Loss: 0.25000089406967163\n",
      "Epoch  64 | Loss: 0.25000080466270447\n",
      "Epoch  65 | Loss: 0.2500007748603821\n",
      "Epoch  66 | Loss: 0.2500007152557373\n",
      "Epoch  67 | Loss: 0.25000065565109253\n",
      "Epoch  68 | Loss: 0.25000062584877014\n",
      "Epoch  69 | Loss: 0.25000059604644775\n",
      "Epoch  70 | Loss: 0.250000536441803\n",
      "Epoch  71 | Loss: 0.2500005066394806\n",
      "Epoch  72 | Loss: 0.2500004768371582\n",
      "Epoch  73 | Loss: 0.2500004172325134\n",
      "Epoch  74 | Loss: 0.2500004172325134\n",
      "Epoch  75 | Loss: 0.25000038743019104\n",
      "Epoch  76 | Loss: 0.25000035762786865\n",
      "Epoch  77 | Loss: 0.25000035762786865\n",
      "Epoch  78 | Loss: 0.25000035762786865\n",
      "Epoch  79 | Loss: 0.2500002682209015\n",
      "Epoch  80 | Loss: 0.2500002682209015\n",
      "Epoch  81 | Loss: 0.2500002384185791\n",
      "Epoch  82 | Loss: 0.2500002086162567\n",
      "Epoch  83 | Loss: 0.2500002086162567\n",
      "Epoch  84 | Loss: 0.2500002086162567\n",
      "Epoch  85 | Loss: 0.2500002086162567\n",
      "Epoch  86 | Loss: 0.2500001788139343\n",
      "Epoch  87 | Loss: 0.2500001788139343\n",
      "Epoch  88 | Loss: 0.25000014901161194\n",
      "Epoch  89 | Loss: 0.25000014901161194\n",
      "Epoch  90 | Loss: 0.25000014901161194\n",
      "Epoch  91 | Loss: 0.25000011920928955\n",
      "Epoch  92 | Loss: 0.25000011920928955\n",
      "Epoch  93 | Loss: 0.25000008940696716\n",
      "Epoch  94 | Loss: 0.25000008940696716\n",
      "Epoch  95 | Loss: 0.25000008940696716\n",
      "Epoch  96 | Loss: 0.25000011920928955\n",
      "Epoch  97 | Loss: 0.2500000596046448\n",
      "Epoch  98 | Loss: 0.25000008940696716\n",
      "Epoch  99 | Loss: 0.2500000596046448\n",
      "tensor([[0.0004, 0.0004]]) tensor([[0.4996]])\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "lr = 0.1\n",
    "\n",
    "w, b = train(n_epochs, lr, w, b, x, y)\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8sMLaJ9a770"
   },
   "source": [
    "이전과는 다르게 loss가 1.0보다 작아지지 않는 것을 알 수 있습니다. 실제 예측 결과를 살펴보면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1723361279283,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "L81iXxgHDIq2",
    "outputId": "1fc0cef5-f24d-45e7-bc0a-fc91c82762b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4864, 0.4981, 0.4981, 0.5098]])\n",
      "tensor([0, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "print(pred(w, b, x))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuqkwJ2NbB7S"
   },
   "source": [
    "보시다시피 0과 1에 해당하는 data들을 잘 구분하지 못하는 모습니다. Linear regression model은 XOR을 잘 처리하지 못하는 것을 우리는 이번 실습을 통해 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1723361279283,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "2zAy7YgFDMgx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN2/ufUFmLdn0BAFVwEnpdC",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (welcomedl)",
   "language": "python",
   "name": "welcomedl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
